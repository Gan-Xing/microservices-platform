# æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡å¼€å‘æŒ‡å— - æ ‡å‡†ç‰ˆæœ¬

## ğŸ“‹ é¡¹ç›®è§„åˆ’é˜¶æ®µ (Project Planning)

### é¡¹ç›®è®¡åˆ’åˆ¶å®š
**å¼€å‘å‘¨æœŸ**: Week 3-4 (æ ‡å‡†ç‰ˆæœ¬4å‘¨è®¡åˆ’)
**ä¼˜å…ˆçº§**: â­â­â­â­ (Week 3-4 æ‰©å±•æœåŠ¡)
**ä¾èµ–å…³ç³»**: ä¾èµ–ç”¨æˆ·ç®¡ç†(3003)ã€è®¤è¯æˆæƒ(3001)ã€APIç½‘å…³(3000)
**å†…å­˜åˆ†é…**: 512MB (æ€»è®¡8GBä¸­çš„åˆ†é…)

### é‡Œç¨‹ç¢‘è®¾ç½®
- **Week 3.1**: åŸºç¡€æ¶ˆæ¯æ“ä½œå®ç° (å‘å¸ƒ/è®¢é˜…)
- **Week 3.2**: Redis Streamsé›†æˆå®Œæˆ
- **Week 3.3**: é˜Ÿåˆ—ç®¡ç†å’Œç›‘æ§åŠŸèƒ½
- **Week 3.4**: å¥åº·æ£€æŸ¥å’ŒæœåŠ¡é›†æˆ
- **Week 4.1**: æ€§èƒ½ä¼˜åŒ–å’Œé”™è¯¯å¤„ç†
- **Week 4.2**: ç»¼åˆæµ‹è¯•å’Œéƒ¨ç½²éªŒè¯

### èµ„æºåˆ†é…
- **ç«¯å£**: 3010
- **æ•°æ®åº“**: å…±äº«PostgreSQLå®ä¾‹ (mq_å¼€å¤´è¡¨)
- **ç¼“å­˜**: å…±äº«Rediså®ä¾‹ (ä¸“ç”¨å‘½åç©ºé—´)
- **å­˜å‚¨éœ€æ±‚**: 100GBæ¶ˆæ¯å­˜å‚¨ç©ºé—´
- **APIç«¯ç‚¹**: 14ä¸ªæ ¸å¿ƒç«¯ç‚¹

### é£é™©è¯„ä¼°
- **æŠ€æœ¯é£é™©**: Redis Streamså­¦ä¹ æ›²çº¿ - ä¸­ç­‰é£é™©
- **ä¾èµ–é£é™©**: éœ€è¦è®¤è¯æœåŠ¡å…ˆå®Œæˆ - ä½é£é™©
- **é›†æˆé£é™©**: ä¸æ‰€æœ‰æœåŠ¡éƒ½æœ‰æ¶ˆæ¯äº¤äº’ - é«˜é£é™©
- **æ€§èƒ½é£é™©**: 10ä¸‡ç”¨æˆ·æ¶ˆæ¯é‡ - ä¸­ç­‰é£é™©

---

## ğŸ¯ éœ€æ±‚åˆ†æé˜¶æ®µ (Requirements Analysis)

### ä¸šåŠ¡éœ€æ±‚æ”¶é›†
æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡æ˜¯å¾®æœåŠ¡å¹³å°çš„æ¶ˆæ¯ä¸­é—´ä»¶æ ¸å¿ƒï¼Œé¢å‘**100ç§Ÿæˆ·+10ä¸‡ç”¨æˆ·**çš„ä¼ä¸šçº§ç”Ÿäº§ç³»ç»Ÿï¼Œè´Ÿè´£å¼‚æ­¥æ¶ˆæ¯ä¼ é€’ã€æœåŠ¡è§£è€¦ã€äº‹ä»¶é©±åŠ¨æ¶æ„å’Œå¯é æ¶ˆæ¯ä¼ è¾“ï¼Œä¸ºæ•´ä¸ªå¹³å°æä¾›é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ¶ˆæ¯é€šä¿¡èƒ½åŠ›ã€‚

### æŠ€æœ¯éœ€æ±‚åˆ†æ
- **æ¶ˆæ¯å¤„ç†**: æ—¥å¤„ç†100ä¸‡æ¡æ¶ˆæ¯ï¼Œæ”¯æŒæ ‡å‡†ç‰ˆæœ¬å¹¶å‘
- **å¯é æ€§**: 99.9%æ¶ˆæ¯å¯é æ€§ï¼Œæ”¯æŒæ¶ˆæ¯æŒä¹…åŒ–
- **å»¶è¿Ÿè¦æ±‚**: æ¶ˆæ¯å»¶è¿Ÿ<10msï¼Œå®æ—¶æ¶ˆæ¯å¤„ç†
- **é˜Ÿåˆ—ç®¡ç†**: æ”¯æŒå¤šç§é˜Ÿåˆ—ç±»å‹ï¼Œæ™ºèƒ½è·¯ç”±åˆ†å‘
- **éƒ¨ç½²æ–¹å¼**: Docker Compose + Redis Streams

### ç”¨æˆ·æ•…äº‹ç¼–å†™
1. **ç³»ç»Ÿç®¡ç†å‘˜**: éœ€è¦åˆ›å»ºå’Œç®¡ç†æ¶ˆæ¯é˜Ÿåˆ—ï¼Œç›‘æ§æ¶ˆæ¯å¤„ç†çŠ¶æ€
2. **åº”ç”¨å¼€å‘è€…**: éœ€è¦å‘å¸ƒæ¶ˆæ¯åˆ°é˜Ÿåˆ—ï¼Œè®¢é˜…é˜Ÿåˆ—æ¶ˆæ¯è¿›è¡Œå¤„ç†
3. **è¿ç»´äººå‘˜**: éœ€è¦ç›‘æ§é˜Ÿåˆ—å¥åº·çŠ¶æ€ï¼ŒæŸ¥çœ‹æ¶ˆæ¯å¤„ç†æŒ‡æ ‡
4. **ç§Ÿæˆ·ç®¡ç†å‘˜**: éœ€è¦ç®¡ç†ç§Ÿæˆ·çº§åˆ«çš„æ¶ˆæ¯é˜Ÿåˆ—é…ç½®å’Œæƒé™

### éªŒæ”¶æ ‡å‡†å®šä¹‰
- **åŠŸèƒ½éªŒæ”¶**: 4ä¸ªæ ¸å¿ƒåŠŸèƒ½æ¨¡å—100%å®ç°
- **æ€§èƒ½éªŒæ”¶**: æ”¯æŒ1000 QPSæ¶ˆæ¯å¤„ç†ï¼ŒP95å»¶è¿Ÿ<50ms
- **å¯é æ€§éªŒæ”¶**: æ¶ˆæ¯ä¸ä¸¢å¤±ï¼Œæ”¯æŒé”™è¯¯é‡è¯•å’Œæ­»ä¿¡é˜Ÿåˆ—
- **é›†æˆéªŒæ”¶**: ä¸å…¶ä»–11ä¸ªæœåŠ¡æ¶ˆæ¯é€šä¿¡æ­£å¸¸

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡é˜¶æ®µ (Architecture Design)

### ç³»ç»Ÿæ¶æ„è®¾è®¡
æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡é‡‡ç”¨**æ ‡å‡†ç‰ˆæœ¬ç®€åŒ–æ¶æ„**ï¼Œä½¿ç”¨Redis Streamsä½œä¸ºä¸»è¦æ¶ˆæ¯é˜Ÿåˆ—ï¼Œé¿å…Kafkaç­‰é‡é‡çº§ç»„ä»¶ï¼š

### æŠ€æœ¯æ¶æ„è¯´æ˜
æ ‡å‡†ç‰ˆæœ¬æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡ä¸“ä¸º100ç§Ÿæˆ·+10ä¸‡ç”¨æˆ·è§„æ¨¡è®¾è®¡ï¼Œé€‰æ‹©æœ€é€‚åˆçš„æŠ€æœ¯æ ˆï¼š

#### æ ‡å‡†ç‰ˆæœ¬æŠ€æœ¯é€‰æ‹© âœ…
- **æ¶ˆæ¯å­˜å‚¨**: Redis Streams (é€‚åˆæ ‡å‡†ç‰ˆæœ¬ååé‡)
- **å…ƒæ•°æ®å­˜å‚¨**: PostgreSQL (å¤ç”¨ç°æœ‰æ•°æ®åº“)
- **æ¡†æ¶**: NestJS 10.x + TypeScript 5.x (ç»Ÿä¸€æŠ€æœ¯æ ˆ)
- **éƒ¨ç½²**: Docker Compose (é¿å…K8Så¤æ‚æ€§)
- **åºåˆ—åŒ–**: JSON (ç®€åŒ–å¤„ç†)

#### ä¼ä¸šç‰ˆæœ¬ä¿ç•™ â­ (æœªæ¥æ‰©å±•)
- **Apache Kafka**: ä¼ä¸šç‰ˆé«˜ååé‡åœºæ™¯
- **æ¶ˆæ¯å‹ç¼©**: Protobuf/Avroç­‰é«˜çº§åºåˆ—åŒ–
- **åˆ†å¸ƒå¼è·Ÿè¸ª**: Jaeger/Zipkiné›†æˆ

### æ•°æ®åº“è®¾è®¡
**PostgreSQLè¡¨ç»“æ„** (å…±äº«æ•°æ®åº“å®ä¾‹)ï¼š

## æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. é˜Ÿåˆ—ç®¡ç†
```typescript
// é˜Ÿåˆ—ç®¡ç†æ¥å£
POST   /api/v1/mq/queues                     // åˆ›å»ºé˜Ÿåˆ—
GET    /api/v1/mq/queues                     // è·å–é˜Ÿåˆ—åˆ—è¡¨
GET    /api/v1/mq/queues/{name}              // è·å–é˜Ÿåˆ—è¯¦æƒ…
PUT    /api/v1/mq/queues/{name}              // æ›´æ–°é˜Ÿåˆ—é…ç½®
DELETE /api/v1/mq/queues/{name}              // åˆ é™¤é˜Ÿåˆ—
POST   /api/v1/mq/queues/{name}/purge        // æ¸…ç©ºé˜Ÿåˆ—
```

### 2. æ¶ˆæ¯å‘å¸ƒ
```typescript
// æ¶ˆæ¯å‘å¸ƒæ¥å£
POST   /api/v1/mq/publish                    // å‘å¸ƒå•æ¡æ¶ˆæ¯
POST   /api/v1/mq/publish/batch              // æ‰¹é‡å‘å¸ƒæ¶ˆæ¯
POST   /api/v1/mq/publish/delayed            // å»¶è¿Ÿæ¶ˆæ¯å‘å¸ƒ
POST   /api/v1/mq/publish/scheduled          // å®šæ—¶æ¶ˆæ¯å‘å¸ƒ
POST   /api/v1/mq/topics/{name}/publish      // å‘å¸ƒåˆ°æŒ‡å®šä¸»é¢˜
```

### 3. æ¶ˆæ¯è®¢é˜…
```typescript
// æ¶ˆæ¯è®¢é˜…ç®¡ç†
POST   /api/v1/mq/subscriptions              // åˆ›å»ºè®¢é˜…
GET    /api/v1/mq/subscriptions              // è·å–è®¢é˜…åˆ—è¡¨
GET    /api/v1/mq/subscriptions/{id}         // è·å–è®¢é˜…è¯¦æƒ…
PUT    /api/v1/mq/subscriptions/{id}         // æ›´æ–°è®¢é˜…
DELETE /api/v1/mq/subscriptions/{id}         // åˆ é™¤è®¢é˜…
POST   /api/v1/mq/subscriptions/{id}/pause   // æš‚åœè®¢é˜…
POST   /api/v1/mq/subscriptions/{id}/resume  // æ¢å¤è®¢é˜…
```

### 4. ç›‘æ§ç»Ÿè®¡
```typescript
// ç›‘æ§ç»Ÿè®¡æ¥å£
GET    /api/v1/mq/metrics                    // è·å–ç³»ç»ŸæŒ‡æ ‡
GET    /api/v1/mq/metrics/topics             // è·å–ä¸»é¢˜æŒ‡æ ‡
GET    /api/v1/mq/metrics/consumers          // è·å–æ¶ˆè´¹è€…æŒ‡æ ‡
GET    /api/v1/mq/health                     // å¥åº·æ£€æŸ¥
GET    /api/v1/mq/status                     // æœåŠ¡çŠ¶æ€
```

## æ•°æ®åº“è®¾è®¡

### ä¸»é¢˜è¡¨ (message_topics)
```sql
CREATE TABLE message_topics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(200) NOT NULL UNIQUE,
  display_name VARCHAR(200),
  description TEXT,
  
  -- ä¸»é¢˜é…ç½®
  topic_type VARCHAR(50) NOT NULL, -- 'kafka', 'redis_stream'
  partition_count INTEGER DEFAULT 1,
  replication_factor INTEGER DEFAULT 1,
  retention_ms BIGINT DEFAULT 604800000, -- 7å¤©
  
  -- æ¶ˆæ¯é…ç½®
  message_format VARCHAR(50) DEFAULT 'json', -- 'json', 'avro', 'protobuf'
  schema_definition JSONB,
  compression_type VARCHAR(20) DEFAULT 'none',
  
  -- è®¿é—®æ§åˆ¶
  tenant_id UUID NOT NULL,
  is_public BOOLEAN DEFAULT FALSE,
  allowed_producers JSONB DEFAULT '[]',
  allowed_consumers JSONB DEFAULT '[]',
  
  -- çŠ¶æ€ç®¡ç†
  status VARCHAR(20) DEFAULT 'active', -- 'active', 'paused', 'archived'
  
  -- æ—¶é—´æˆ³
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);
```

### è®¢é˜…è¡¨ (message_subscriptions)
```sql
CREATE TABLE message_subscriptions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  topic_id UUID REFERENCES message_topics(id) ON DELETE CASCADE,
  
  -- è®¢é˜…ä¿¡æ¯
  consumer_group VARCHAR(200) NOT NULL,
  consumer_name VARCHAR(200) NOT NULL,
  subscription_type VARCHAR(50) NOT NULL, -- 'kafka_consumer', 'redis_consumer', 'webhook'
  
  -- æ¶ˆè´¹é…ç½®
  offset_reset_policy VARCHAR(20) DEFAULT 'latest', -- 'earliest', 'latest'
  max_poll_records INTEGER DEFAULT 500,
  session_timeout_ms INTEGER DEFAULT 30000,
  heartbeat_interval_ms INTEGER DEFAULT 3000,
  
  -- æ¶ˆæ¯å¤„ç†
  handler_config JSONB NOT NULL, -- å¤„ç†å™¨é…ç½®
  dead_letter_queue VARCHAR(200),
  max_retry_attempts INTEGER DEFAULT 3,
  retry_delay_ms INTEGER DEFAULT 1000,
  
  -- è¿‡æ»¤å™¨
  message_filter JSONB, -- æ¶ˆæ¯è¿‡æ»¤æ¡ä»¶
  
  -- çŠ¶æ€ç®¡ç†
  status VARCHAR(20) DEFAULT 'active', -- 'active', 'paused', 'stopped'
  tenant_id UUID NOT NULL,
  
  -- æ—¶é—´æˆ³
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  last_consumed_at TIMESTAMP,
  
  UNIQUE(topic_id, consumer_group, consumer_name)
);
```

### æ¶ˆæ¯è®°å½•è¡¨ (message_records)
```sql
CREATE TABLE message_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  topic_id UUID REFERENCES message_topics(id),
  
  -- æ¶ˆæ¯æ ‡è¯†
  message_key VARCHAR(500),
  message_id VARCHAR(200) UNIQUE NOT NULL,
  correlation_id VARCHAR(200),
  
  -- æ¶ˆæ¯å†…å®¹
  headers JSONB DEFAULT '{}',
  payload JSONB,
  payload_size INTEGER,
  content_type VARCHAR(100) DEFAULT 'application/json',
  
  -- è·¯ç”±ä¿¡æ¯
  partition_id INTEGER,
  offset_value BIGINT,
  
  -- æ¶ˆæ¯çŠ¶æ€
  status VARCHAR(20) DEFAULT 'published', -- 'published', 'consumed', 'failed', 'dead_letter'
  retry_count INTEGER DEFAULT 0,
  
  -- æ—¶é—´ä¿¡æ¯
  published_at TIMESTAMP DEFAULT NOW(),
  scheduled_at TIMESTAMP,
  consumed_at TIMESTAMP,
  expires_at TIMESTAMP,
  
  -- å…ƒæ•°æ®
  producer_id VARCHAR(200),
  tenant_id UUID NOT NULL,
  trace_id VARCHAR(100)
);
```

### æ¶ˆè´¹è®°å½•è¡¨ (consumption_records)
```sql
CREATE TABLE consumption_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  subscription_id UUID REFERENCES message_subscriptions(id),
  message_id VARCHAR(200) NOT NULL,
  
  -- æ¶ˆè´¹ä¿¡æ¯
  consumer_instance VARCHAR(200) NOT NULL,
  partition_id INTEGER,
  offset_value BIGINT,
  
  -- å¤„ç†ç»“æœ
  status VARCHAR(20) NOT NULL, -- 'success', 'failed', 'retrying'
  processing_time_ms INTEGER,
  error_message TEXT,
  retry_count INTEGER DEFAULT 0,
  
  -- æ—¶é—´æˆ³
  consumed_at TIMESTAMP DEFAULT NOW(),
  processed_at TIMESTAMP,
  next_retry_at TIMESTAMP,
  
  -- å…ƒæ•°æ®
  tenant_id UUID NOT NULL
);
```

### æ ‡å‡†ç‰ˆæœ¬æ¶ˆæ¯æ¶æ„
**ç®€åŒ–æ¶æ„** - ä¸“æ³¨Redis Streamsï¼Œé¿å…è¿‡åº¦å¤æ‚æ€§ï¼š

```mermaid
graph TB
    Producer[æ¶ˆæ¯ç”Ÿäº§è€…] --> Gateway[APIç½‘å…³:3000]
    Gateway --> MQService[æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡:3010]
    
    MQService --> RedisStreams[Redis Streams]
    MQService --> PostgreSQL[PostgreSQLå…ƒæ•°æ®]
    
    RedisStreams --> ConsumerGroup[æ¶ˆè´¹è€…ç»„]
    ConsumerGroup --> MessageProcessor[æ¶ˆæ¯å¤„ç†å™¨]
    
    MessageProcessor --> UserService[ç”¨æˆ·æœåŠ¡:3003]
    MessageProcessor --> NotificationService[é€šçŸ¥æœåŠ¡:3005]
    MessageProcessor --> AuditService[å®¡è®¡æœåŠ¡:3008]
    
    MessageProcessor --> DeadLetter[æ­»ä¿¡é˜Ÿåˆ—]
    MessageProcessor --> Monitoring[ç›‘æ§æœåŠ¡:3007]
```

### æœåŠ¡é—´äº¤äº’è®¾è®¡
åŸºäºSERVICE_INTERACTION_SPEC.mdï¼Œæ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡ä¸å…¶ä»–æœåŠ¡çš„äº¤äº’æ¨¡å¼ï¼š

#### å†…éƒ¨APIæ¥å£ (æœåŠ¡é—´é€šä¿¡)
```typescript
// å†…éƒ¨æœåŠ¡è°ƒç”¨ - ä½¿ç”¨X-Service-Tokenè®¤è¯
interface InternalMessageAPI {
  // ç”¨æˆ·æœåŠ¡è°ƒç”¨
  'POST /internal/mq/user-events',     // ç”¨æˆ·äº‹ä»¶å‘å¸ƒ
  'POST /internal/mq/user-notifications', // ç”¨æˆ·é€šçŸ¥æ¶ˆæ¯
  
  // å®¡è®¡æœåŠ¡è°ƒç”¨
  'POST /internal/mq/audit-logs',      // å®¡è®¡æ—¥å¿—æ¶ˆæ¯
  'GET /internal/mq/audit-queue-status', // å®¡è®¡é˜Ÿåˆ—çŠ¶æ€
  
  // é€šçŸ¥æœåŠ¡è°ƒç”¨
  'POST /internal/mq/notifications',   // é€šçŸ¥æ¶ˆæ¯å‘å¸ƒ
  'GET /internal/mq/notification-queue', // é€šçŸ¥é˜Ÿåˆ—çŠ¶æ€
  
  // ç›‘æ§æœåŠ¡è°ƒç”¨
  'GET /internal/mq/metrics',          // æ¶ˆæ¯é˜Ÿåˆ—æŒ‡æ ‡
  'GET /internal/mq/health-detailed'   // è¯¦ç»†å¥åº·çŠ¶æ€
}
```

#### ç»Ÿä¸€é”™è¯¯å¤„ç†
```typescript
// æ ‡å‡†ç‰ˆæœ¬é”™è¯¯å“åº”æ ¼å¼
interface MessageQueueError {
  code: 'MQ_QUEUE_NOT_FOUND' | 'MQ_PUBLISH_FAILED' | 'MQ_CONSUMER_ERROR';
  message: string;
  details?: any;
  retryAfter?: number; // é‡è¯•å»¶è¿Ÿ(ç§’)
}
```
```

## æ ¸å¿ƒå®ç° - æ ‡å‡†ç‰ˆæœ¬

### Redis Streamsæ¶ˆæ¯å‘å¸ƒ
```typescript
@Injectable()
export class MessagePublisher {
  constructor(
    private readonly redis: Redis,
    private readonly auditService: AuditService // æœåŠ¡é—´è°ƒç”¨
  ) {}
  
  async publishMessage(request: PublishMessageRequest): Promise<PublishMessageResponse> {
    const stream = `mq:${request.queue}`;
    const messageData = {
      id: generateId(),
      payload: JSON.stringify(request.payload),
      headers: JSON.stringify(request.headers || {}),
      tenant_id: request.tenantId,
      created_at: Date.now().toString()
    };
    
    try {
      // å‘å¸ƒåˆ°Redis Stream
      const messageId = await this.redis.xadd(
        stream,
        '*', // è‡ªåŠ¨ç”ŸæˆID
        ...Object.entries(messageData).flat()
      );
      
      // è®°å½•å®¡è®¡æ—¥å¿— (æœåŠ¡é—´è°ƒç”¨)
      await this.auditService.logOperation({
        operation: 'message_published',
        resource: `queue:${request.queue}`,
        tenantId: request.tenantId,
        details: { messageId, size: JSON.stringify(request.payload).length }
      });
      
      return {
        messageId,
        queue: request.queue,
        publishedAt: new Date()
      };
    } catch (error) {
      throw new MessagePublishException(
        `Failed to publish message to queue ${request.queue}`,
        error
      );
    }
  }
}
```

### Redis Streamsæ¶ˆæ¯æ¶ˆè´¹
```typescript
@Injectable()
export class MessageConsumer {
  private activeConsumers: Map<string, boolean> = new Map();
  
  constructor(
    private readonly redis: Redis,
    private readonly notificationService: NotificationService // æœåŠ¡é—´è°ƒç”¨
  ) {}
  
  async createConsumerGroup(queue: string, group: string): Promise<void> {
    const stream = `mq:${queue}`;
    try {
      await this.redis.xgroup('CREATE', stream, group, '0', 'MKSTREAM');
    } catch (error) {
      // å¿½ç•¥å·²å­˜åœ¨é”™è¯¯
      if (!error.message.includes('BUSYGROUP')) {
        throw error;
      }
    }
  }
  
  async startConsumer(config: ConsumerConfig): Promise<void> {
    const consumerKey = `${config.queue}:${config.group}:${config.consumer}`;
    const stream = `mq:${config.queue}`;
    
    if (this.activeConsumers.get(consumerKey)) {
      throw new Error(`Consumer ${consumerKey} already running`);
    }
    
    this.activeConsumers.set(consumerKey, true);
    
    // æ¶ˆè´¹å¾ªç¯
    while (this.activeConsumers.get(consumerKey)) {
      try {
        const results = await this.redis.xreadgroup(
          'GROUP', config.group, config.consumer,
          'COUNT', 10,
          'BLOCK', 1000,
          'STREAMS', stream, '>'
        );
        
        if (results && results.length > 0) {
          const [, messages] = results[0];
          await this.processMessages(messages, config);
        }
        
      } catch (error) {
        console.error('Consumer error:', error);
        await this.delay(5000);
      }
    }
  }
  
  private async processMessages(
    messages: Array<[string, string[]]>,
    config: ConsumerConfig
  ): Promise<void> {
    for (const [messageId, fields] of messages) {
      try {
        const messageData = this.parseFields(fields);
        
        // æ ¹æ®æ¶ˆæ¯ç±»å‹è·¯ç”±åˆ°å¯¹åº”æœåŠ¡
        await this.routeMessage(messageData, config);
        
        // ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
        await this.redis.xack(`mq:${config.queue}`, config.group, messageId);
        
      } catch (error) {
        console.error('Message processing error:', error);
        // æ¶ˆæ¯ä¿æŒpendingçŠ¶æ€ç­‰å¾…é‡è¯•
      }
    }
  }
  
  private async routeMessage(message: any, config: ConsumerConfig): Promise<void> {
    const payload = JSON.parse(message.payload);
    
    // æ ¹æ®æ¶ˆæ¯ç±»å‹è°ƒç”¨å¯¹åº”æœåŠ¡
    switch (payload.type) {
      case 'user_notification':
        await this.notificationService.sendNotification(payload.data);
        break;
      case 'audit_log':
        await this.auditService.createLog(payload.data);
        break;
      default:
        console.warn(`Unknown message type: ${payload.type}`);
    }
  }
}
```

## Redis Streamså®ç°

### Redisç”Ÿäº§è€…
```typescript
@Injectable()
export class RedisProducerService {
  constructor(private readonly redis: Redis) {}
  
  async publishToStream(
    stream: string, 
    data: Record<string, any>,
    options?: StreamPublishOptions
  ): Promise<string> {
    const fields = this.flattenObject(data);
    
    const messageId = await this.redis.xadd(
      stream,
      options?.messageId || '*', // ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ID
      ...Object.entries(fields).flat()
    );
    
    // è®¾ç½®æµçš„æœ€å¤§é•¿åº¦ï¼ˆå¯é€‰ï¼‰
    if (options?.maxLength) {
      await this.redis.xtrim(stream, 'MAXLEN', '~', options.maxLength);
    }
    
    return messageId;
  }
  
  async publishBatchToStream(
    stream: string,
    messages: Array<Record<string, any>>
  ): Promise<string[]> {
    const pipeline = this.redis.pipeline();
    
    messages.forEach(message => {
      const fields = this.flattenObject(message);
      pipeline.xadd(stream, '*', ...Object.entries(fields).flat());
    });
    
    const results = await pipeline.exec();
    return results.map(result => result[1] as string);
  }
  
  private flattenObject(obj: Record<string, any>): Record<string, string> {
    const flattened: Record<string, string> = {};
    
    for (const [key, value] of Object.entries(obj)) {
      if (typeof value === 'object' && value !== null) {
        flattened[key] = JSON.stringify(value);
      } else {
        flattened[key] = String(value);
      }
    }
    
    return flattened;
  }
}
```

### Redisæ¶ˆè´¹è€…
```typescript
@Injectable()
export class RedisConsumerService {
  private activeConsumers: Map<string, boolean> = new Map();
  
  async createConsumerGroup(
    stream: string,
    group: string,
    startId: string = '0'
  ): Promise<void> {
    try {
      await this.redis.xgroup('CREATE', stream, group, startId, 'MKSTREAM');
    } catch (error) {
      // å¿½ç•¥å·²å­˜åœ¨çš„é”™è¯¯
      if (!error.message.includes('BUSYGROUP')) {
        throw error;
      }
    }
  }
  
  async startConsumer(config: RedisConsumerConfig): Promise<void> {
    const consumerKey = `${config.stream}:${config.group}:${config.consumer}`;
    
    if (this.activeConsumers.get(consumerKey)) {
      throw new Error(`Consumer ${consumerKey} is already running`);
    }
    
    this.activeConsumers.set(consumerKey, true);
    
    // å¯åŠ¨æ¶ˆè´¹å¾ªç¯
    this.consumeLoop(config);
  }
  
  private async consumeLoop(config: RedisConsumerConfig): Promise<void> {
    const consumerKey = `${config.stream}:${config.group}:${config.consumer}`;
    
    while (this.activeConsumers.get(consumerKey)) {
      try {
        // è¯»å–æ–°æ¶ˆæ¯
        const results = await this.redis.xreadgroup(
          'GROUP', config.group, config.consumer,
          'COUNT', config.batchSize || 10,
          'BLOCK', config.blockTimeMs || 1000,
          'STREAMS', config.stream, '>'
        );
        
        if (results && results.length > 0) {
          const [, messages] = results[0];
          await this.processMessages(messages, config);
        }
        
        // å¤„ç†å¾…ç¡®è®¤çš„æ¶ˆæ¯
        await this.processPendingMessages(config);
        
      } catch (error) {
        console.error(`Redis consumer error:`, error);
        await this.delay(config.errorDelayMs || 5000);
      }
    }
  }
  
  private async processMessages(
    messages: Array<[string, string[]]>,
    config: RedisConsumerConfig
  ): Promise<void> {
    for (const [messageId, fields] of messages) {
      try {
        const data = this.parseFields(fields);
        await this.executeHandler(data, config.handlerConfig);
        
        // ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
        await this.redis.xack(config.stream, config.group, messageId);
        
      } catch (error) {
        console.error(`Message processing error:`, error);
        // ä¸ç¡®è®¤æ¶ˆæ¯ï¼Œè®©å®ƒä¿æŒåœ¨pendingçŠ¶æ€
      }
    }
  }
  
  private async processPendingMessages(config: RedisConsumerConfig): Promise<void> {
    const pendingMessages = await this.redis.xpending(
      config.stream,
      config.group,
      '-', '+', 10 // è·å–æœ€å¤š10æ¡pendingæ¶ˆæ¯
    );
    
    for (const pending of pendingMessages) {
      const [messageId, , idleTime] = pending;
      
      // å¦‚æœæ¶ˆæ¯ç©ºé—²æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œé‡æ–°å¤„ç†
      if (idleTime > (config.maxIdleTimeMs || 60000)) {
        try {
          const messages = await this.redis.xclaim(
            config.stream,
            config.group,
            config.consumer,
            config.maxIdleTimeMs || 60000,
            messageId
          );
          
          if (messages.length > 0) {
            await this.processMessages(messages, config);
          }
        } catch (error) {
          console.error(`Pending message processing error:`, error);
        }
      }
    }
  }
  
  private parseFields(fields: string[]): Record<string, any> {
    const data: Record<string, any> = {};
    
    for (let i = 0; i < fields.length; i += 2) {
      const key = fields[i];
      const value = fields[i + 1];
      
      try {
        // å°è¯•è§£æJSON
        data[key] = JSON.parse(value);
      } catch {
        // å¦‚æœä¸æ˜¯JSONï¼Œä¿æŒå­—ç¬¦ä¸²
        data[key] = value;
      }
    }
    
    return data;
  }
}
```

## æ¶ˆæ¯å¤„ç†å™¨æ¡†æ¶

### å¤„ç†å™¨æ³¨å†Œ
```typescript
@Injectable()
export class MessageHandlerRegistry {
  private handlers: Map<string, MessageHandler> = new Map();
  
  registerHandler(type: string, handler: MessageHandler): void {
    this.handlers.set(type, handler);
  }
  
  async executeHandler(
    message: Message,
    config: HandlerConfig
  ): Promise<void> {
    const handler = this.handlers.get(config.handlerType);
    
    if (!handler) {
      throw new Error(`No handler found for type: ${config.handlerType}`);
    }
    
    // æ‰§è¡Œå‰ç½®å¤„ç†
    await this.preProcess(message, config);
    
    // æ‰§è¡Œä¸»å¤„ç†é€»è¾‘
    const result = await handler.handle(message, config);
    
    // æ‰§è¡Œåç½®å¤„ç†
    await this.postProcess(message, result, config);
  }
  
  private async preProcess(message: Message, config: HandlerConfig): Promise<void> {
    // æ¶ˆæ¯éªŒè¯
    if (config.schema) {
      await this.validateMessage(message, config.schema);
    }
    
    // æ¶ˆæ¯è¿‡æ»¤
    if (config.filter) {
      const shouldProcess = await this.applyFilter(message, config.filter);
      if (!shouldProcess) {
        throw new MessageFilteredException('Message filtered out');
      }
    }
    
    // æ¶ˆæ¯è½¬æ¢
    if (config.transformer) {
      await this.transformMessage(message, config.transformer);
    }
  }
}
```

### å…·ä½“å¤„ç†å™¨å®ç°
```typescript
// HTTPå›è°ƒå¤„ç†å™¨
@Injectable()
export class HttpCallbackHandler implements MessageHandler {
  async handle(message: Message, config: HttpCallbackConfig): Promise<any> {
    const response = await this.httpService.post(config.url, {
      headers: {
        'Content-Type': 'application/json',
        ...config.headers
      },
      body: message.payload,
      timeout: config.timeoutMs || 30000
    });
    
    if (response.status >= 400) {
      throw new Error(`HTTP callback failed: ${response.status}`);
    }
    
    return response.data;
  }
}

// å‡½æ•°å¤„ç†å™¨
@Injectable()
export class FunctionHandler implements MessageHandler {
  async handle(message: Message, config: FunctionConfig): Promise<any> {
    const module = await import(config.modulePath);
    const func = module[config.functionName];
    
    if (typeof func !== 'function') {
      throw new Error(`Function ${config.functionName} not found`);
    }
    
    return await func(message.payload, message.headers);
  }
}

// æ•°æ®åº“å¤„ç†å™¨
@Injectable()
export class DatabaseHandler implements MessageHandler {
  async handle(message: Message, config: DatabaseConfig): Promise<any> {
    const entity = this.mapMessageToEntity(message.payload, config.mapping);
    
    switch (config.operation) {
      case 'insert':
        return await this.repository.insert(entity);
      case 'update':
        return await this.repository.update(config.criteria, entity);
      case 'delete':
        return await this.repository.delete(config.criteria);
      default:
        throw new Error(`Unsupported operation: ${config.operation}`);
    }
  }
}
```

## æ¶ˆæ¯åºåˆ—åŒ–

### å¤šæ ¼å¼æ”¯æŒ
```typescript
@Injectable()
export class MessageSerializer {
  serialize(data: any, format: SerializationFormat): Buffer {
    switch (format) {
      case SerializationFormat.JSON:
        return Buffer.from(JSON.stringify(data));
        
      case SerializationFormat.AVRO:
        return this.avroSerializer.serialize(data);
        
      case SerializationFormat.PROTOBUF:
        return this.protobufSerializer.serialize(data);
        
      case SerializationFormat.MSGPACK:
        return msgpack.encode(data);
        
      default:
        throw new Error(`Unsupported serialization format: ${format}`);
    }
  }
  
  deserialize(buffer: Buffer, format: SerializationFormat): any {
    switch (format) {
      case SerializationFormat.JSON:
        return JSON.parse(buffer.toString());
        
      case SerializationFormat.AVRO:
        return this.avroSerializer.deserialize(buffer);
        
      case SerializationFormat.PROTOBUF:
        return this.protobufSerializer.deserialize(buffer);
        
      case SerializationFormat.MSGPACK:
        return msgpack.decode(buffer);
        
      default:
        throw new Error(`Unsupported serialization format: ${format}`);
    }
  }
}
```

### Schemaç®¡ç†
```typescript
@Injectable()
export class SchemaRegistry {
  private schemas: Map<string, any> = new Map();
  
  async registerSchema(
    subject: string,
    schema: any,
    format: SchemaFormat
  ): Promise<string> {
    const schemaId = this.generateSchemaId(subject, schema);
    
    // éªŒè¯schema
    await this.validateSchema(schema, format);
    
    // æ£€æŸ¥å…¼å®¹æ€§
    await this.checkCompatibility(subject, schema);
    
    // å­˜å‚¨schema
    this.schemas.set(schemaId, {
      subject,
      schema,
      format,
      version: await this.getNextVersion(subject),
      createdAt: new Date()
    });
    
    return schemaId;
  }
  
  async getSchema(schemaId: string): Promise<any> {
    const schemaInfo = this.schemas.get(schemaId);
    if (!schemaInfo) {
      throw new Error(`Schema not found: ${schemaId}`);
    }
    return schemaInfo.schema;
  }
}
```

## ç›‘æ§å’ŒæŒ‡æ ‡

### ç³»ç»ŸæŒ‡æ ‡
```typescript
interface MessageQueueMetrics {
  // KafkaæŒ‡æ ‡
  kafkaTopics: {
    totalTopics: number;
    totalPartitions: number;
    totalMessages: number;
    messagesPerSecond: number;
    bytesPerSecond: number;
  };
  
  // RedisæŒ‡æ ‡
  redisStreams: {
    totalStreams: number;
    totalConsumerGroups: number;
    totalMessages: number;
    pendingMessages: number;
  };
  
  // æ¶ˆè´¹è€…æŒ‡æ ‡
  consumers: {
    activeConsumers: number;
    totalLag: number;
    processingRate: number;
    errorRate: number;
  };
  
  // æ€§èƒ½æŒ‡æ ‡
  performance: {
    averageLatency: number;
    p95Latency: number;
    p99Latency: number;
    throughput: number;
  };
}

@Injectable()
export class MetricsCollector {
  private prometheusRegistry = new Registry();
  
  constructor() {
    this.setupMetrics();
  }
  
  private setupMetrics(): void {
    // æ¶ˆæ¯è®¡æ•°å™¨
    this.messageCounter = new Counter({
      name: 'mq_messages_total',
      help: 'Total number of messages',
      labelNames: ['topic', 'type', 'status', 'tenant_id'],
      registers: [this.prometheusRegistry]
    });
    
    // æ¶ˆæ¯å»¶è¿Ÿç›´æ–¹å›¾
    this.latencyHistogram = new Histogram({
      name: 'mq_message_latency_seconds',
      help: 'Message processing latency',
      labelNames: ['topic', 'handler_type'],
      buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],
      registers: [this.prometheusRegistry]
    });
    
    // é˜Ÿåˆ—å¤§å°gauge
    this.queueSizeGauge = new Gauge({
      name: 'mq_queue_size',
      help: 'Current queue size',
      labelNames: ['queue_name', 'type'],
      registers: [this.prometheusRegistry]
    });
  }
  
  recordMessage(
    topic: string,
    type: string,
    status: 'published' | 'consumed' | 'failed',
    tenantId: string
  ): void {
    this.messageCounter.inc({
      topic,
      type,
      status,
      tenant_id: tenantId
    });
  }
  
  recordLatency(
    topic: string,
    handlerType: string,
    latencySeconds: number
  ): void {
    this.latencyHistogram.observe(
      { topic, handler_type: handlerType },
      latencySeconds
    );
  }
}
```

### å¥åº·æ£€æŸ¥
```typescript
@Injectable()
export class HealthCheckService {
  async checkKafkaHealth(): Promise<HealthStatus> {
    try {
      const admin = this.kafka.admin();
      await admin.connect();
      
      const metadata = await admin.fetchTopicMetadata();
      await admin.disconnect();
      
      return {
        status: 'healthy',
        details: {
          brokers: metadata.brokers.length,
          topics: metadata.topics.length
        }
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }
  
  async checkRedisHealth(): Promise<HealthStatus> {
    try {
      const pong = await this.redis.ping();
      const info = await this.redis.info('memory');
      
      return {
        status: pong === 'PONG' ? 'healthy' : 'unhealthy',
        details: {
          ping: pong,
          memory: this.parseRedisInfo(info)
        }
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }
}
```

## é”™è¯¯å¤„ç†å’Œé‡è¯•

### æ­»ä¿¡é˜Ÿåˆ—
```typescript
@Injectable()
export class DeadLetterQueueService {
  async sendToDeadLetter(
    originalMessage: Message,
    error: Error,
    config: ConsumerConfig
  ): Promise<void> {
    const deadLetterMessage = {
      originalTopic: originalMessage.topic,
      originalPartition: originalMessage.partition,
      originalOffset: originalMessage.offset,
      originalTimestamp: originalMessage.timestamp,
      failureReason: error.message,
      failureStack: error.stack,
      retryCount: originalMessage.retryCount || 0,
      payload: originalMessage.payload,
      headers: originalMessage.headers,
      failedAt: new Date().toISOString()
    };
    
    const deadLetterTopic = config.deadLetterQueue || `${originalMessage.topic}.deadletter`;
    
    await this.publishMessage(deadLetterTopic, deadLetterMessage);
    
    // è®°å½•æ­»ä¿¡æ¶ˆæ¯
    await this.recordDeadLetterMessage(deadLetterMessage);
  }
  
  async reprocessDeadLetterMessages(
    deadLetterTopic: string,
    maxMessages: number = 100
  ): Promise<void> {
    const messages = await this.consumeMessages(deadLetterTopic, maxMessages);
    
    for (const message of messages) {
      try {
        const originalTopic = message.payload.originalTopic;
        const originalPayload = message.payload.payload;
        
        // é‡æ–°å‘å¸ƒåˆ°åŸå§‹ä¸»é¢˜
        await this.publishMessage(originalTopic, originalPayload);
        
        // ç¡®è®¤æ­»ä¿¡æ¶ˆæ¯å¤„ç†å®Œæˆ
        await this.acknowledgeMessage(message);
        
      } catch (error) {
        console.error('Failed to reprocess dead letter message:', error);
      }
    }
  }
}
```

## APIè®¾è®¡è§„èŒƒ

### æ¶ˆæ¯å‘å¸ƒæ¥å£
```typescript
interface PublishMessageRequest {
  topic: string;
  messages: Array<{
    key?: string;
    payload: any;
    headers?: Record<string, string>;
    partition?: number;
    timestamp?: number;
  }>;
  options?: {
    compression?: 'none' | 'gzip' | 'snappy' | 'lz4';
    acks?: 'all' | 0 | 1;
    timeout?: number;
  };
}

interface PublishMessageResponse {
  messageIds: string[];
  partitions: Array<{
    partition: number;
    offset: number;
  }>;
  timestamp: number;
}
```

### é”™è¯¯å“åº”æ ¼å¼
```typescript
enum MessageQueueErrorCode {
  TOPIC_NOT_FOUND = 'TOPIC_NOT_FOUND',
  SUBSCRIPTION_NOT_FOUND = 'SUBSCRIPTION_NOT_FOUND',
  SERIALIZATION_ERROR = 'SERIALIZATION_ERROR',
  SCHEMA_VALIDATION_ERROR = 'SCHEMA_VALIDATION_ERROR',
  QUOTA_EXCEEDED = 'QUOTA_EXCEEDED',
  CONNECTION_ERROR = 'CONNECTION_ERROR',
  BROKER_NOT_AVAILABLE = 'BROKER_NOT_AVAILABLE'
}
```

## æ ‡å‡†ç‰ˆæœ¬éƒ¨ç½²é…ç½®

### Docker Composeé…ç½®
**æ ‡å‡†ç‰ˆæœ¬**: å…±äº«åŸºç¡€è®¾æ–½ï¼Œé¿å…ç‹¬ç«‹ç»„ä»¶

```yaml
# docker-compose.yml (é¡¹ç›®æ ¹ç›®å½•)
version: '3.8'
services:
  message-queue-service:
    build: ./message-queue-service
    ports:
      - "3010:3010"
    environment:
      # å…±äº«æ•°æ®åº“è¿æ¥
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/platform
      # å…±äº«Redisè¿æ¥
      - REDIS_URL=redis://redis:6379/5  # ä¸“ç”¨æ•°æ®åº“5
      # æœåŠ¡å‘ç° (Docker Composeå†…ç½®)
      - AUTH_SERVICE_URL=http://auth-service:3001
      - USER_SERVICE_URL=http://user-management-service:3003
      - AUDIT_SERVICE_URL=http://audit-service:3008
      - NOTIFICATION_SERVICE_URL=http://notification-service:3005
      # æ€§èƒ½é…ç½®
      - MESSAGE_BATCH_SIZE=50
      - MAX_CONCURRENT_CONSUMERS=5
      - PROCESSING_TIMEOUT_MS=15000
    depends_on:
      - postgres
      - redis
      - auth-service
    networks:
      - platform-network
      
  # å…±äº«åŸºç¡€è®¾æ–½
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=platform
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
      
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
  
networks:
  platform-network:
    driver: bridge
```

### ç¯å¢ƒå˜é‡é…ç½®
```env
# æ ‡å‡†ç‰ˆæœ¬ç¯å¢ƒå˜é‡
NODE_ENV=production
PORT=3010

# å…±äº«æ•°æ®åº“
DATABASE_URL=postgresql://postgres:password@postgres:5432/platform

# ä¸“ç”¨Rediså‘½åç©ºé—´
REDIS_URL=redis://redis:6379/5
REDIS_KEY_PREFIX=mq:

# æœåŠ¡é—´é€šä¿¡
INTERNAL_SERVICE_TOKEN=your-internal-service-token
SERVICE_DISCOVERY_MODE=docker-compose

# æ€§èƒ½é…ç½® (æ ‡å‡†ç‰ˆæœ¬ä¼˜åŒ–)
MAX_MESSAGE_SIZE=1048576  # 1MB
DEFAULT_TTL=604800       # 7å¤©
MAX_QUEUE_LENGTH=10000
CONSUMER_TIMEOUT=30000

# ç›‘æ§é…ç½®
METRICS_ENABLED=true
HEALTH_CHECK_INTERVAL=30
```

### å¥åº·æ£€æŸ¥é›†æˆ
```typescript
@Controller('/health')
export class HealthController {
  constructor(
    private readonly redis: Redis,
    private readonly prisma: PrismaService
  ) {}
  
  @Get()
  async healthCheck(): Promise<HealthStatus> {
    const checks = await Promise.allSettled([
      this.checkRedis(),
      this.checkDatabase(),
      this.checkQueues()
    ]);
    
    const status = checks.every(check => 
      check.status === 'fulfilled' && check.value.healthy
    ) ? 'healthy' : 'unhealthy';
    
    return {
      service: 'message-queue-service',
      status,
      timestamp: new Date(),
      checks: {
        redis: checks[0].status === 'fulfilled' ? checks[0].value : { healthy: false },
        database: checks[1].status === 'fulfilled' ? checks[1].value : { healthy: false },
        queues: checks[2].status === 'fulfilled' ? checks[2].value : { healthy: false }
      }
    };
  }
  
  private async checkRedis(): Promise<{ healthy: boolean; details?: any }> {
    try {
      const pong = await this.redis.ping();
      return { 
        healthy: pong === 'PONG',
        details: { latency: Date.now() }
      };
    } catch (error) {
      return { healthy: false, details: { error: error.message } };
    }
  }
}

## æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```typescript
@Injectable()
export class BatchProcessor {
  private batches: Map<string, MessageBatch> = new Map();
  
  async addToBatch(topic: string, message: Message): Promise<void> {
    let batch = this.batches.get(topic);
    
    if (!batch) {
      batch = new MessageBatch(topic, this.config.batchSize);
      this.batches.set(topic, batch);
    }
    
    batch.addMessage(message);
    
    if (batch.isFull() || batch.isExpired()) {
      await this.processBatch(batch);
      this.batches.delete(topic);
    }
  }
  
  private async processBatch(batch: MessageBatch): Promise<void> {
    try {
      await this.publishBatch(batch.topic, batch.messages);
      batch.markAsProcessed();
    } catch (error) {
      batch.markAsFailed(error);
      await this.handleBatchError(batch, error);
    }
  }
}
```

### è¿æ¥æ± ç®¡ç†
```typescript
@Injectable()
export class ConnectionPoolManager {
  private kafkaProducers: Pool<Producer>;
  private redisConnections: Pool<Redis>;
  
  constructor() {
    this.kafkaProducers = new Pool({
      create: () => this.createKafkaProducer(),
      destroy: (producer) => producer.disconnect(),
      min: 2,
      max: 10,
      acquireTimeoutMillis: 30000,
      idleTimeoutMillis: 300000
    });
    
    this.redisConnections = new Pool({
      create: () => this.createRedisConnection(),
      destroy: (redis) => redis.disconnect(),
      min: 5,
      max: 20,
      acquireTimeoutMillis: 10000,
      idleTimeoutMillis: 60000
    });
  }
  
  async withKafkaProducer<T>(operation: (producer: Producer) => Promise<T>): Promise<T> {
    const producer = await this.kafkaProducers.acquire();
    try {
      return await operation(producer);
    } finally {
      this.kafkaProducers.release(producer);
    }
  }
}
```

## å®‰å…¨è€ƒè™‘

### æ¶ˆæ¯åŠ å¯†
```typescript
@Injectable()
export class MessageEncryption {
  async encryptMessage(payload: any, tenantKey: string): Promise<string> {
    const cipher = crypto.createCipher('aes-256-gcm', tenantKey);
    const encrypted = cipher.update(JSON.stringify(payload), 'utf8', 'hex') + 
                     cipher.final('hex');
    const authTag = cipher.getAuthTag();
    
    return Buffer.from(JSON.stringify({
      data: encrypted,
      authTag: authTag.toString('hex')
    })).toString('base64');
  }
  
  async decryptMessage(encryptedPayload: string, tenantKey: string): Promise<any> {
    const { data, authTag } = JSON.parse(
      Buffer.from(encryptedPayload, 'base64').toString()
    );
    
    const decipher = crypto.createDecipher('aes-256-gcm', tenantKey);
    decipher.setAuthTag(Buffer.from(authTag, 'hex'));
    
    const decrypted = decipher.update(data, 'hex', 'utf8') + decipher.final('utf8');
    return JSON.parse(decrypted);
  }
}
```

### è®¿é—®æ§åˆ¶
```typescript
@Injectable()
export class MessageAccessControl {
  async checkPublishPermission(
    userId: string,
    topic: string,
    tenantId: string
  ): Promise<boolean> {
    const topicConfig = await this.getTopicConfig(topic);
    
    // æ£€æŸ¥ç§Ÿæˆ·æƒé™
    if (topicConfig.tenantId !== tenantId) {
      return false;
    }
    
    // æ£€æŸ¥ç”Ÿäº§è€…æƒé™
    if (topicConfig.allowedProducers.length > 0) {
      return topicConfig.allowedProducers.includes(userId);
    }
    
    return true;
  }
  
  async checkConsumePermission(
    userId: string,
    topic: string,
    tenantId: string
  ): Promise<boolean> {
    const topicConfig = await this.getTopicConfig(topic);
    
    // æ£€æŸ¥ç§Ÿæˆ·æƒé™
    if (topicConfig.tenantId !== tenantId && !topicConfig.isPublic) {
      return false;
    }
    
    // æ£€æŸ¥æ¶ˆè´¹è€…æƒé™
    if (topicConfig.allowedConsumers.length > 0) {
      return topicConfig.allowedConsumers.includes(userId);
    }
    
    return true;
  }
}
```

---

## âœ… å¼€å‘å®Œæˆæƒ…å†µæ€»ç»“

### 1.1 éœ€æ±‚åˆ†æé˜¶æ®µ âœ… å·²å®Œæˆ
- âœ… ä¸šåŠ¡éœ€æ±‚æ”¶é›†ï¼šæ˜ç¡®æ¶ˆæ¯é˜Ÿåˆ—åœ¨å¾®æœåŠ¡æ¶æ„ä¸­çš„æ ¸å¿ƒä½œç”¨
- âœ… æŠ€æœ¯éœ€æ±‚åˆ†æï¼šå®šä¹‰100ç§Ÿæˆ·+10ä¸‡ç”¨æˆ·æ€§èƒ½æŒ‡æ ‡
- âœ… ç”¨æˆ·æ•…äº‹ç¼–å†™ï¼šè¦†ç›–ç®¡ç†å‘˜ã€å¼€å‘è€…ã€è¿ç»´äººå‘˜ä½¿ç”¨åœºæ™¯
- âœ… éªŒæ”¶æ ‡å‡†å®šä¹‰ï¼šåŠŸèƒ½ã€æ€§èƒ½ã€å¯é æ€§ã€é›†æˆå››ä¸ªç»´åº¦æ ‡å‡†
- âœ… æ¶æ„è®¾è®¡æ–‡æ¡£ï¼šåŸºäºRedis Streamsçš„æ ‡å‡†ç‰ˆæœ¬æ¶æ„

### 1.2 é¡¹ç›®è§„åˆ’é˜¶æ®µ âœ… å·²å®Œæˆ
- âœ… é¡¹ç›®è®¡åˆ’åˆ¶å®šï¼šWeek 3-4å¼€å‘è®¡åˆ’ï¼Œä¼˜å…ˆçº§â­â­â­â­
- âœ… é‡Œç¨‹ç¢‘è®¾ç½®ï¼š6ä¸ªæ˜ç¡®çš„å‘¨çº§é‡Œç¨‹ç¢‘å’Œäº¤ä»˜èŠ‚ç‚¹
- âœ… èµ„æºåˆ†é…ï¼šç«¯å£3010ã€512MBå†…å­˜ã€100GBå­˜å‚¨ã€14ä¸ªAPIç«¯ç‚¹
- âœ… é£é™©è¯„ä¼°ï¼šæŠ€æœ¯ã€ä¾èµ–ã€é›†æˆã€æ€§èƒ½å››ä¸ªç»´åº¦é£é™©åˆ†æ
- âœ… æŠ€æœ¯æ ˆé€‰æ‹©ï¼šRedis Streams + PostgreSQLï¼Œç¬¦åˆæ ‡å‡†ç‰ˆæœ¬è¦æ±‚

### 1.3 æ¶æ„è®¾è®¡é˜¶æ®µ âœ… å·²å®Œæˆ
- âœ… ç³»ç»Ÿæ¶æ„è®¾è®¡ï¼šç®€åŒ–çš„Redis Streamsæ¶æ„ï¼Œé¿å…Kafkaå¤æ‚æ€§
- âœ… æ•°æ®åº“è®¾è®¡ï¼šå®Œæ•´çš„PostgreSQLè¡¨ç»“æ„è®¾è®¡(4ä¸ªæ ¸å¿ƒè¡¨)
- âœ… APIè®¾è®¡ï¼š14ä¸ªRESTfulæ¥å£ï¼Œæ¶µç›–4ä¸ªåŠŸèƒ½æ¨¡å—
- âœ… å®‰å…¨æ¶æ„è®¾è®¡ï¼šæœåŠ¡é—´è®¤è¯ã€æ¶ˆæ¯åŠ å¯†ã€è®¿é—®æ§åˆ¶
- âœ… æ€§èƒ½è§„åˆ’ï¼šé’ˆå¯¹æ ‡å‡†ç‰ˆæœ¬è§„æ¨¡çš„æ‰¹é‡å¤„ç†å’Œè¿æ¥æ± è®¾è®¡

## ğŸš€ ä¸»è¦æ”¹è¿›ç‚¹

### æŠ€æœ¯æ¶æ„ä¼˜åŒ–
1. **ç§»é™¤è¿‡åº¦å¤æ‚æ€§**ï¼šä»Kafkaæ··åˆæ¶æ„ç®€åŒ–ä¸ºRedis Streamså•ä¸€æ¶æ„
2. **ç»Ÿä¸€åŸºç¡€è®¾æ–½**ï¼šå…±äº«PostgreSQLå’ŒRediså®ä¾‹ï¼Œé™ä½è¿ç»´å¤æ‚åº¦
3. **Docker Composeä¼˜åŒ–**ï¼šé¿å…K8Sï¼Œä½¿ç”¨å®¹å™¨ç¼–æ’è¿›è¡ŒæœåŠ¡å‘ç°

### æœåŠ¡é›†æˆå¢å¼º
1. **å†…éƒ¨APIè®¾è®¡**ï¼šå®šä¹‰ä¸å…¶ä»–11ä¸ªæœåŠ¡çš„æ¶ˆæ¯äº¤äº’æ¥å£
2. **ç»Ÿä¸€é”™è¯¯å¤„ç†**ï¼šæ ‡å‡†åŒ–é”™è¯¯å“åº”æ ¼å¼å’Œé‡è¯•æœºåˆ¶
3. **å¥åº·æ£€æŸ¥é›†æˆ**ï¼šä¸ç›‘æ§æœåŠ¡(3007)æ·±åº¦é›†æˆ

### æ ‡å‡†ç‰ˆæœ¬é€‚é…
1. **æ€§èƒ½ç›®æ ‡æ˜ç¡®**ï¼šæ—¥å¤„ç†100ä¸‡æ¶ˆæ¯ï¼Œæ”¯æŒ1000 QPS
2. **èµ„æºé…ç½®ä¼˜åŒ–**ï¼š512MBå†…å­˜åˆ†é…ï¼Œé€‚åˆ8GBæ€»å†…å­˜é™åˆ¶
3. **éƒ¨ç½²ç®€åŒ–**ï¼šå•ä¸€Docker Composeæ–‡ä»¶ï¼Œé¿å…å¤šç»„ä»¶ä¾èµ–

é€šè¿‡æ ‡å‡†ç‰ˆæœ¬ä¼˜åŒ–ï¼Œæ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡ç°åœ¨å…·å¤‡äº†ç”Ÿäº§çº§åˆ«çš„æ¶æ„è®¾è®¡ã€æ˜ç¡®çš„å¼€å‘è·¯å¾„å’Œå®Œæ•´çš„é›†æˆæ–¹æ¡ˆï¼Œèƒ½å¤Ÿåœ¨4å‘¨å¼€å‘è®¡åˆ’ä¸­é«˜è´¨é‡äº¤ä»˜ã€‚