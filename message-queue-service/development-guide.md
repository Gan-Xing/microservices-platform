# æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡å¼€å‘æ–‡æ¡£ - æ ‡å‡†ç‰ˆæœ¬

## æœåŠ¡æ¦‚è¿°

æ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡æ˜¯å¾®æœåŠ¡å¹³å°çš„æ¶ˆæ¯ä¸­é—´ä»¶æ ¸å¿ƒï¼Œé¢å‘**100ç§Ÿæˆ·+10ä¸‡ç”¨æˆ·**çš„ä¼ä¸šçº§ç”Ÿäº§ç³»ç»Ÿï¼Œè´Ÿè´£å¼‚æ­¥æ¶ˆæ¯ä¼ é€’ã€æœåŠ¡è§£è€¦ã€äº‹ä»¶é©±åŠ¨æ¶æ„å’Œå¯é æ¶ˆæ¯ä¼ è¾“ï¼Œä¸ºæ•´ä¸ªå¹³å°æä¾›é«˜æ€§èƒ½ã€é«˜å¯ç”¨çš„æ¶ˆæ¯é€šä¿¡èƒ½åŠ›ã€‚

### ğŸ¯ æ ‡å‡†ç‰ˆæœ¬å®šä½
- **æ¶ˆæ¯å¤„ç†**: æ—¥å¤„ç†100ä¸‡æ¡æ¶ˆæ¯ï¼Œæ”¯æŒæ ‡å‡†ç‰ˆæœ¬å¹¶å‘
- **å¯é æ€§**: 99.9%æ¶ˆæ¯å¯é æ€§ï¼Œæ”¯æŒæ¶ˆæ¯æŒä¹…åŒ–
- **å»¶è¿Ÿè¦æ±‚**: æ¶ˆæ¯å»¶è¿Ÿ<10msï¼Œå®æ—¶æ¶ˆæ¯å¤„ç†
- **é˜Ÿåˆ—ç®¡ç†**: æ”¯æŒå¤šç§é˜Ÿåˆ—ç±»å‹ï¼Œæ™ºèƒ½è·¯ç”±åˆ†å‘
- **éƒ¨ç½²æ–¹å¼**: Docker Compose + Redis Streams

## æŠ€æœ¯æ ˆ

### åç«¯æŠ€æœ¯ (æ ‡å‡†ç‰ˆæœ¬)
- **æ¡†æ¶**: NestJS 10.x + TypeScript 5.x
- **æ•°æ®åº“**: PostgreSQL 15+ (å…ƒæ•°æ®) + Redis 7+ (æ¶ˆæ¯å­˜å‚¨)
- **ORM**: Prisma ORM
- **æ¶ˆæ¯ä¸­é—´ä»¶**: Redis Streams (ä¸»è¦) + Redis Pub/Sub (è¾…åŠ©)
- **å®¢æˆ·ç«¯**: ioredis + Redis Bull Queue
- **ä¼ä¸šç‰ˆåŠŸèƒ½**: Apache Kafka 3.5+ (ä¼ä¸šç‰ˆä¿ç•™)

### æ¶ˆæ¯æŠ€æœ¯ (æ ‡å‡†ç‰ˆæœ¬)
- **Redis Streams**: ä¸»è¦æ¶ˆæ¯é˜Ÿåˆ— (é€‚åˆæ ‡å‡†ç‰ˆæœ¬è§„æ¨¡)
- **Redis Pub/Sub**: å®æ—¶é€šçŸ¥ã€è½»é‡çº§æ¶ˆæ¯
- **åè°ƒå™¨**: Redis åˆ†å¸ƒå¼é” + æ¶ˆè´¹è€…ç»„
- **ç›‘æ§**: Redis Commander + Custom Metrics
- **åºåˆ—åŒ–**: JSON (ç®€åŒ–åºåˆ—åŒ–)

## æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. é˜Ÿåˆ—ç®¡ç†
```typescript
// é˜Ÿåˆ—ç®¡ç†æ¥å£
POST   /api/v1/mq/queues                     // åˆ›å»ºé˜Ÿåˆ—
GET    /api/v1/mq/queues                     // è·å–é˜Ÿåˆ—åˆ—è¡¨
GET    /api/v1/mq/queues/{name}              // è·å–é˜Ÿåˆ—è¯¦æƒ…
PUT    /api/v1/mq/queues/{name}              // æ›´æ–°é˜Ÿåˆ—é…ç½®
DELETE /api/v1/mq/queues/{name}              // åˆ é™¤é˜Ÿåˆ—
POST   /api/v1/mq/queues/{name}/purge        // æ¸…ç©ºé˜Ÿåˆ—
```

### 2. æ¶ˆæ¯å‘å¸ƒ
```typescript
// æ¶ˆæ¯å‘å¸ƒæ¥å£
POST   /api/v1/mq/publish                    // å‘å¸ƒå•æ¡æ¶ˆæ¯
POST   /api/v1/mq/publish/batch              // æ‰¹é‡å‘å¸ƒæ¶ˆæ¯
POST   /api/v1/mq/publish/delayed            // å»¶è¿Ÿæ¶ˆæ¯å‘å¸ƒ
POST   /api/v1/mq/publish/scheduled          // å®šæ—¶æ¶ˆæ¯å‘å¸ƒ
POST   /api/v1/mq/topics/{name}/publish      // å‘å¸ƒåˆ°æŒ‡å®šä¸»é¢˜
```

### 3. æ¶ˆæ¯è®¢é˜…
```typescript
// æ¶ˆæ¯è®¢é˜…ç®¡ç†
POST   /api/v1/mq/subscriptions              // åˆ›å»ºè®¢é˜…
GET    /api/v1/mq/subscriptions              // è·å–è®¢é˜…åˆ—è¡¨
GET    /api/v1/mq/subscriptions/{id}         // è·å–è®¢é˜…è¯¦æƒ…
PUT    /api/v1/mq/subscriptions/{id}         // æ›´æ–°è®¢é˜…
DELETE /api/v1/mq/subscriptions/{id}         // åˆ é™¤è®¢é˜…
POST   /api/v1/mq/subscriptions/{id}/pause   // æš‚åœè®¢é˜…
POST   /api/v1/mq/subscriptions/{id}/resume  // æ¢å¤è®¢é˜…
```

### 4. ç›‘æ§ç»Ÿè®¡
```typescript
// ç›‘æ§ç»Ÿè®¡æ¥å£
GET    /api/v1/mq/metrics                    // è·å–ç³»ç»ŸæŒ‡æ ‡
GET    /api/v1/mq/metrics/topics             // è·å–ä¸»é¢˜æŒ‡æ ‡
GET    /api/v1/mq/metrics/consumers          // è·å–æ¶ˆè´¹è€…æŒ‡æ ‡
GET    /api/v1/mq/health                     // å¥åº·æ£€æŸ¥
GET    /api/v1/mq/status                     // æœåŠ¡çŠ¶æ€
```

## æ•°æ®åº“è®¾è®¡

### ä¸»é¢˜è¡¨ (message_topics)
```sql
CREATE TABLE message_topics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  name VARCHAR(200) NOT NULL UNIQUE,
  display_name VARCHAR(200),
  description TEXT,
  
  -- ä¸»é¢˜é…ç½®
  topic_type VARCHAR(50) NOT NULL, -- 'kafka', 'redis_stream'
  partition_count INTEGER DEFAULT 1,
  replication_factor INTEGER DEFAULT 1,
  retention_ms BIGINT DEFAULT 604800000, -- 7å¤©
  
  -- æ¶ˆæ¯é…ç½®
  message_format VARCHAR(50) DEFAULT 'json', -- 'json', 'avro', 'protobuf'
  schema_definition JSONB,
  compression_type VARCHAR(20) DEFAULT 'none',
  
  -- è®¿é—®æ§åˆ¶
  tenant_id UUID NOT NULL,
  is_public BOOLEAN DEFAULT FALSE,
  allowed_producers JSONB DEFAULT '[]',
  allowed_consumers JSONB DEFAULT '[]',
  
  -- çŠ¶æ€ç®¡ç†
  status VARCHAR(20) DEFAULT 'active', -- 'active', 'paused', 'archived'
  
  -- æ—¶é—´æˆ³
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);
```

### è®¢é˜…è¡¨ (message_subscriptions)
```sql
CREATE TABLE message_subscriptions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  topic_id UUID REFERENCES message_topics(id) ON DELETE CASCADE,
  
  -- è®¢é˜…ä¿¡æ¯
  consumer_group VARCHAR(200) NOT NULL,
  consumer_name VARCHAR(200) NOT NULL,
  subscription_type VARCHAR(50) NOT NULL, -- 'kafka_consumer', 'redis_consumer', 'webhook'
  
  -- æ¶ˆè´¹é…ç½®
  offset_reset_policy VARCHAR(20) DEFAULT 'latest', -- 'earliest', 'latest'
  max_poll_records INTEGER DEFAULT 500,
  session_timeout_ms INTEGER DEFAULT 30000,
  heartbeat_interval_ms INTEGER DEFAULT 3000,
  
  -- æ¶ˆæ¯å¤„ç†
  handler_config JSONB NOT NULL, -- å¤„ç†å™¨é…ç½®
  dead_letter_queue VARCHAR(200),
  max_retry_attempts INTEGER DEFAULT 3,
  retry_delay_ms INTEGER DEFAULT 1000,
  
  -- è¿‡æ»¤å™¨
  message_filter JSONB, -- æ¶ˆæ¯è¿‡æ»¤æ¡ä»¶
  
  -- çŠ¶æ€ç®¡ç†
  status VARCHAR(20) DEFAULT 'active', -- 'active', 'paused', 'stopped'
  tenant_id UUID NOT NULL,
  
  -- æ—¶é—´æˆ³
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  last_consumed_at TIMESTAMP,
  
  UNIQUE(topic_id, consumer_group, consumer_name)
);
```

### æ¶ˆæ¯è®°å½•è¡¨ (message_records)
```sql
CREATE TABLE message_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  topic_id UUID REFERENCES message_topics(id),
  
  -- æ¶ˆæ¯æ ‡è¯†
  message_key VARCHAR(500),
  message_id VARCHAR(200) UNIQUE NOT NULL,
  correlation_id VARCHAR(200),
  
  -- æ¶ˆæ¯å†…å®¹
  headers JSONB DEFAULT '{}',
  payload JSONB,
  payload_size INTEGER,
  content_type VARCHAR(100) DEFAULT 'application/json',
  
  -- è·¯ç”±ä¿¡æ¯
  partition_id INTEGER,
  offset_value BIGINT,
  
  -- æ¶ˆæ¯çŠ¶æ€
  status VARCHAR(20) DEFAULT 'published', -- 'published', 'consumed', 'failed', 'dead_letter'
  retry_count INTEGER DEFAULT 0,
  
  -- æ—¶é—´ä¿¡æ¯
  published_at TIMESTAMP DEFAULT NOW(),
  scheduled_at TIMESTAMP,
  consumed_at TIMESTAMP,
  expires_at TIMESTAMP,
  
  -- å…ƒæ•°æ®
  producer_id VARCHAR(200),
  tenant_id UUID NOT NULL,
  trace_id VARCHAR(100)
);
```

### æ¶ˆè´¹è®°å½•è¡¨ (consumption_records)
```sql
CREATE TABLE consumption_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  subscription_id UUID REFERENCES message_subscriptions(id),
  message_id VARCHAR(200) NOT NULL,
  
  -- æ¶ˆè´¹ä¿¡æ¯
  consumer_instance VARCHAR(200) NOT NULL,
  partition_id INTEGER,
  offset_value BIGINT,
  
  -- å¤„ç†ç»“æœ
  status VARCHAR(20) NOT NULL, -- 'success', 'failed', 'retrying'
  processing_time_ms INTEGER,
  error_message TEXT,
  retry_count INTEGER DEFAULT 0,
  
  -- æ—¶é—´æˆ³
  consumed_at TIMESTAMP DEFAULT NOW(),
  processed_at TIMESTAMP,
  next_retry_at TIMESTAMP,
  
  -- å…ƒæ•°æ®
  tenant_id UUID NOT NULL
);
```

## æ¶ˆæ¯æ¶æ„è®¾è®¡

### Kafka + Redis æ··åˆæ¶æ„
```mermaid
graph TB
    Producer[æ¶ˆæ¯ç”Ÿäº§è€…] --> Router[æ¶ˆæ¯è·¯ç”±å™¨]
    Router --> KafkaCheck{æ¶ˆæ¯ç±»å‹åˆ¤æ–­}
    Router --> RedisCheck{æ¶ˆæ¯ç±»å‹åˆ¤æ–­}
    
    KafkaCheck -->|ä¸šåŠ¡äº‹ä»¶| KafkaCluster[Kafkaé›†ç¾¤]
    KafkaCheck -->|æ•°æ®æµ| KafkaCluster
    KafkaCheck -->|å¤§æ‰¹é‡æ¶ˆæ¯| KafkaCluster
    
    RedisCheck -->|å®æ—¶é€šçŸ¥| RedisStreams[Redis Streams]
    RedisCheck -->|ç³»ç»Ÿæ¶ˆæ¯| RedisStreams
    RedisCheck -->|è½»é‡æ¶ˆæ¯| RedisStreams
    
    KafkaCluster --> KafkaConsumer[Kafkaæ¶ˆè´¹è€…]
    RedisStreams --> RedisConsumer[Redisæ¶ˆè´¹è€…]
    
    KafkaConsumer --> MessageProcessor[æ¶ˆæ¯å¤„ç†å™¨]
    RedisConsumer --> MessageProcessor
    
    MessageProcessor --> DeadLetter[æ­»ä¿¡é˜Ÿåˆ—]
    MessageProcessor --> Metrics[ç›‘æ§æŒ‡æ ‡]
    MessageProcessor --> AuditLog[å®¡è®¡æ—¥å¿—]
```

### æ¶ˆæ¯è·¯ç”±ç­–ç•¥
```typescript
@Injectable()
export class MessageRouter {
  routeMessage(message: Message): MessageBackend {
    // åŸºäºæ¶ˆæ¯ç±»å‹è·¯ç”±
    if (this.isHighThroughputMessage(message)) {
      return MessageBackend.KAFKA;
    }
    
    // åŸºäºæ¶ˆæ¯å¤§å°è·¯ç”±
    if (message.size > this.config.LARGE_MESSAGE_THRESHOLD) {
      return MessageBackend.KAFKA;
    }
    
    // åŸºäºæŒä¹…åŒ–è¦æ±‚è·¯ç”±
    if (message.headers.persistence === 'durable') {
      return MessageBackend.KAFKA;
    }
    
    // åŸºäºå®æ—¶æ€§è¦æ±‚è·¯ç”±
    if (message.headers.priority === 'realtime') {
      return MessageBackend.REDIS;
    }
    
    // é»˜è®¤ä½¿ç”¨Rediså¤„ç†è½»é‡æ¶ˆæ¯
    return MessageBackend.REDIS;
  }
  
  private isHighThroughputMessage(message: Message): boolean {
    const highThroughputTypes = [
      'user.behavior',
      'system.metrics',
      'audit.log',
      'data.sync'
    ];
    
    return highThroughputTypes.includes(message.type);
  }
}
```

## Kafkaé›†æˆå®ç°

### Kafkaç”Ÿäº§è€…
```typescript
@Injectable()
export class KafkaProducerService {
  private kafka: Kafka;
  private producer: Producer;
  
  constructor() {
    this.kafka = new Kafka({
      clientId: 'message-queue-service',
      brokers: process.env.KAFKA_BROKERS.split(','),
      retry: {
        initialRetryTime: 100,
        retries: 8,
        maxRetryTime: 30000,
        retryOnFailure: true
      }
    });
    
    this.producer = this.kafka.producer({
      transactionTimeout: 30000,
      maxInFlightRequests: 5,
      idempotent: true, // ç¡®ä¿å¹‚ç­‰æ€§
      compression: CompressionTypes.GZIP
    });
  }
  
  async publishMessage(topic: string, message: MessagePayload): Promise<void> {
    await this.producer.send({
      topic,
      messages: [{
        key: message.key,
        value: JSON.stringify(message.payload),
        headers: message.headers,
        partition: message.partition,
        timestamp: message.timestamp?.toString()
      }]
    });
  }
  
  async publishBatch(topic: string, messages: MessagePayload[]): Promise<void> {
    const kafkaMessages = messages.map(msg => ({
      key: msg.key,
      value: JSON.stringify(msg.payload),
      headers: msg.headers,
      partition: msg.partition
    }));
    
    await this.producer.sendBatch({
      topicMessages: [{
        topic,
        messages: kafkaMessages
      }]
    });
  }
  
  async publishWithTransaction(
    operations: Array<{ topic: string; messages: MessagePayload[] }>
  ): Promise<void> {
    const transaction = await this.producer.transaction();
    
    try {
      for (const operation of operations) {
        await transaction.send({
          topic: operation.topic,
          messages: operation.messages.map(msg => ({
            key: msg.key,
            value: JSON.stringify(msg.payload),
            headers: msg.headers
          }))
        });
      }
      
      await transaction.commit();
    } catch (error) {
      await transaction.abort();
      throw error;
    }
  }
}
```

### Kafkaæ¶ˆè´¹è€…
```typescript
@Injectable()
export class KafkaConsumerService {
  private kafka: Kafka;
  private consumers: Map<string, Consumer> = new Map();
  
  async createConsumer(config: KafkaConsumerConfig): Promise<void> {
    const consumer = this.kafka.consumer({
      groupId: config.consumerGroup,
      sessionTimeout: config.sessionTimeoutMs || 30000,
      heartbeatInterval: config.heartbeatIntervalMs || 3000,
      maxWaitTimeInMs: 5000,
      allowAutoTopicCreation: false
    });
    
    await consumer.subscribe({
      topics: config.topics,
      fromBeginning: config.offsetResetPolicy === 'earliest'
    });
    
    await consumer.run({
      autoCommit: false,
      partitionsConsumedConcurrently: config.maxConcurrency || 1,
      eachMessage: async ({ topic, partition, message }) => {
        await this.processMessage({
          topic,
          partition,
          offset: message.offset,
          key: message.key?.toString(),
          value: message.value?.toString(),
          headers: message.headers,
          timestamp: message.timestamp
        }, config);
      }
    });
    
    this.consumers.set(config.consumerGroup, consumer);
  }
  
  private async processMessage(
    message: KafkaMessage, 
    config: KafkaConsumerConfig
  ): Promise<void> {
    const startTime = Date.now();
    let retryCount = 0;
    
    while (retryCount <= config.maxRetryAttempts) {
      try {
        await this.executeHandler(message, config.handlerConfig);
        
        // è®°å½•æˆåŠŸæ¶ˆè´¹
        await this.recordConsumption(message, 'success', Date.now() - startTime);
        
        // æäº¤offset
        await this.commitMessage(message, config);
        break;
        
      } catch (error) {
        retryCount++;
        
        if (retryCount > config.maxRetryAttempts) {
          // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
          await this.sendToDeadLetterQueue(message, error, config);
          await this.recordConsumption(message, 'failed', Date.now() - startTime, error);
        } else {
          // ç­‰å¾…é‡è¯•
          await this.delay(config.retryDelayMs * Math.pow(2, retryCount - 1));
        }
      }
    }
  }
}
```

## Redis Streamså®ç°

### Redisç”Ÿäº§è€…
```typescript
@Injectable()
export class RedisProducerService {
  constructor(private readonly redis: Redis) {}
  
  async publishToStream(
    stream: string, 
    data: Record<string, any>,
    options?: StreamPublishOptions
  ): Promise<string> {
    const fields = this.flattenObject(data);
    
    const messageId = await this.redis.xadd(
      stream,
      options?.messageId || '*', // ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„ID
      ...Object.entries(fields).flat()
    );
    
    // è®¾ç½®æµçš„æœ€å¤§é•¿åº¦ï¼ˆå¯é€‰ï¼‰
    if (options?.maxLength) {
      await this.redis.xtrim(stream, 'MAXLEN', '~', options.maxLength);
    }
    
    return messageId;
  }
  
  async publishBatchToStream(
    stream: string,
    messages: Array<Record<string, any>>
  ): Promise<string[]> {
    const pipeline = this.redis.pipeline();
    
    messages.forEach(message => {
      const fields = this.flattenObject(message);
      pipeline.xadd(stream, '*', ...Object.entries(fields).flat());
    });
    
    const results = await pipeline.exec();
    return results.map(result => result[1] as string);
  }
  
  private flattenObject(obj: Record<string, any>): Record<string, string> {
    const flattened: Record<string, string> = {};
    
    for (const [key, value] of Object.entries(obj)) {
      if (typeof value === 'object' && value !== null) {
        flattened[key] = JSON.stringify(value);
      } else {
        flattened[key] = String(value);
      }
    }
    
    return flattened;
  }
}
```

### Redisæ¶ˆè´¹è€…
```typescript
@Injectable()
export class RedisConsumerService {
  private activeConsumers: Map<string, boolean> = new Map();
  
  async createConsumerGroup(
    stream: string,
    group: string,
    startId: string = '0'
  ): Promise<void> {
    try {
      await this.redis.xgroup('CREATE', stream, group, startId, 'MKSTREAM');
    } catch (error) {
      // å¿½ç•¥å·²å­˜åœ¨çš„é”™è¯¯
      if (!error.message.includes('BUSYGROUP')) {
        throw error;
      }
    }
  }
  
  async startConsumer(config: RedisConsumerConfig): Promise<void> {
    const consumerKey = `${config.stream}:${config.group}:${config.consumer}`;
    
    if (this.activeConsumers.get(consumerKey)) {
      throw new Error(`Consumer ${consumerKey} is already running`);
    }
    
    this.activeConsumers.set(consumerKey, true);
    
    // å¯åŠ¨æ¶ˆè´¹å¾ªç¯
    this.consumeLoop(config);
  }
  
  private async consumeLoop(config: RedisConsumerConfig): Promise<void> {
    const consumerKey = `${config.stream}:${config.group}:${config.consumer}`;
    
    while (this.activeConsumers.get(consumerKey)) {
      try {
        // è¯»å–æ–°æ¶ˆæ¯
        const results = await this.redis.xreadgroup(
          'GROUP', config.group, config.consumer,
          'COUNT', config.batchSize || 10,
          'BLOCK', config.blockTimeMs || 1000,
          'STREAMS', config.stream, '>'
        );
        
        if (results && results.length > 0) {
          const [, messages] = results[0];
          await this.processMessages(messages, config);
        }
        
        // å¤„ç†å¾…ç¡®è®¤çš„æ¶ˆæ¯
        await this.processPendingMessages(config);
        
      } catch (error) {
        console.error(`Redis consumer error:`, error);
        await this.delay(config.errorDelayMs || 5000);
      }
    }
  }
  
  private async processMessages(
    messages: Array<[string, string[]]>,
    config: RedisConsumerConfig
  ): Promise<void> {
    for (const [messageId, fields] of messages) {
      try {
        const data = this.parseFields(fields);
        await this.executeHandler(data, config.handlerConfig);
        
        // ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
        await this.redis.xack(config.stream, config.group, messageId);
        
      } catch (error) {
        console.error(`Message processing error:`, error);
        // ä¸ç¡®è®¤æ¶ˆæ¯ï¼Œè®©å®ƒä¿æŒåœ¨pendingçŠ¶æ€
      }
    }
  }
  
  private async processPendingMessages(config: RedisConsumerConfig): Promise<void> {
    const pendingMessages = await this.redis.xpending(
      config.stream,
      config.group,
      '-', '+', 10 // è·å–æœ€å¤š10æ¡pendingæ¶ˆæ¯
    );
    
    for (const pending of pendingMessages) {
      const [messageId, , idleTime] = pending;
      
      // å¦‚æœæ¶ˆæ¯ç©ºé—²æ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼Œé‡æ–°å¤„ç†
      if (idleTime > (config.maxIdleTimeMs || 60000)) {
        try {
          const messages = await this.redis.xclaim(
            config.stream,
            config.group,
            config.consumer,
            config.maxIdleTimeMs || 60000,
            messageId
          );
          
          if (messages.length > 0) {
            await this.processMessages(messages, config);
          }
        } catch (error) {
          console.error(`Pending message processing error:`, error);
        }
      }
    }
  }
  
  private parseFields(fields: string[]): Record<string, any> {
    const data: Record<string, any> = {};
    
    for (let i = 0; i < fields.length; i += 2) {
      const key = fields[i];
      const value = fields[i + 1];
      
      try {
        // å°è¯•è§£æJSON
        data[key] = JSON.parse(value);
      } catch {
        // å¦‚æœä¸æ˜¯JSONï¼Œä¿æŒå­—ç¬¦ä¸²
        data[key] = value;
      }
    }
    
    return data;
  }
}
```

## æ¶ˆæ¯å¤„ç†å™¨æ¡†æ¶

### å¤„ç†å™¨æ³¨å†Œ
```typescript
@Injectable()
export class MessageHandlerRegistry {
  private handlers: Map<string, MessageHandler> = new Map();
  
  registerHandler(type: string, handler: MessageHandler): void {
    this.handlers.set(type, handler);
  }
  
  async executeHandler(
    message: Message,
    config: HandlerConfig
  ): Promise<void> {
    const handler = this.handlers.get(config.handlerType);
    
    if (!handler) {
      throw new Error(`No handler found for type: ${config.handlerType}`);
    }
    
    // æ‰§è¡Œå‰ç½®å¤„ç†
    await this.preProcess(message, config);
    
    // æ‰§è¡Œä¸»å¤„ç†é€»è¾‘
    const result = await handler.handle(message, config);
    
    // æ‰§è¡Œåç½®å¤„ç†
    await this.postProcess(message, result, config);
  }
  
  private async preProcess(message: Message, config: HandlerConfig): Promise<void> {
    // æ¶ˆæ¯éªŒè¯
    if (config.schema) {
      await this.validateMessage(message, config.schema);
    }
    
    // æ¶ˆæ¯è¿‡æ»¤
    if (config.filter) {
      const shouldProcess = await this.applyFilter(message, config.filter);
      if (!shouldProcess) {
        throw new MessageFilteredException('Message filtered out');
      }
    }
    
    // æ¶ˆæ¯è½¬æ¢
    if (config.transformer) {
      await this.transformMessage(message, config.transformer);
    }
  }
}
```

### å…·ä½“å¤„ç†å™¨å®ç°
```typescript
// HTTPå›è°ƒå¤„ç†å™¨
@Injectable()
export class HttpCallbackHandler implements MessageHandler {
  async handle(message: Message, config: HttpCallbackConfig): Promise<any> {
    const response = await this.httpService.post(config.url, {
      headers: {
        'Content-Type': 'application/json',
        ...config.headers
      },
      body: message.payload,
      timeout: config.timeoutMs || 30000
    });
    
    if (response.status >= 400) {
      throw new Error(`HTTP callback failed: ${response.status}`);
    }
    
    return response.data;
  }
}

// å‡½æ•°å¤„ç†å™¨
@Injectable()
export class FunctionHandler implements MessageHandler {
  async handle(message: Message, config: FunctionConfig): Promise<any> {
    const module = await import(config.modulePath);
    const func = module[config.functionName];
    
    if (typeof func !== 'function') {
      throw new Error(`Function ${config.functionName} not found`);
    }
    
    return await func(message.payload, message.headers);
  }
}

// æ•°æ®åº“å¤„ç†å™¨
@Injectable()
export class DatabaseHandler implements MessageHandler {
  async handle(message: Message, config: DatabaseConfig): Promise<any> {
    const entity = this.mapMessageToEntity(message.payload, config.mapping);
    
    switch (config.operation) {
      case 'insert':
        return await this.repository.insert(entity);
      case 'update':
        return await this.repository.update(config.criteria, entity);
      case 'delete':
        return await this.repository.delete(config.criteria);
      default:
        throw new Error(`Unsupported operation: ${config.operation}`);
    }
  }
}
```

## æ¶ˆæ¯åºåˆ—åŒ–

### å¤šæ ¼å¼æ”¯æŒ
```typescript
@Injectable()
export class MessageSerializer {
  serialize(data: any, format: SerializationFormat): Buffer {
    switch (format) {
      case SerializationFormat.JSON:
        return Buffer.from(JSON.stringify(data));
        
      case SerializationFormat.AVRO:
        return this.avroSerializer.serialize(data);
        
      case SerializationFormat.PROTOBUF:
        return this.protobufSerializer.serialize(data);
        
      case SerializationFormat.MSGPACK:
        return msgpack.encode(data);
        
      default:
        throw new Error(`Unsupported serialization format: ${format}`);
    }
  }
  
  deserialize(buffer: Buffer, format: SerializationFormat): any {
    switch (format) {
      case SerializationFormat.JSON:
        return JSON.parse(buffer.toString());
        
      case SerializationFormat.AVRO:
        return this.avroSerializer.deserialize(buffer);
        
      case SerializationFormat.PROTOBUF:
        return this.protobufSerializer.deserialize(buffer);
        
      case SerializationFormat.MSGPACK:
        return msgpack.decode(buffer);
        
      default:
        throw new Error(`Unsupported serialization format: ${format}`);
    }
  }
}
```

### Schemaç®¡ç†
```typescript
@Injectable()
export class SchemaRegistry {
  private schemas: Map<string, any> = new Map();
  
  async registerSchema(
    subject: string,
    schema: any,
    format: SchemaFormat
  ): Promise<string> {
    const schemaId = this.generateSchemaId(subject, schema);
    
    // éªŒè¯schema
    await this.validateSchema(schema, format);
    
    // æ£€æŸ¥å…¼å®¹æ€§
    await this.checkCompatibility(subject, schema);
    
    // å­˜å‚¨schema
    this.schemas.set(schemaId, {
      subject,
      schema,
      format,
      version: await this.getNextVersion(subject),
      createdAt: new Date()
    });
    
    return schemaId;
  }
  
  async getSchema(schemaId: string): Promise<any> {
    const schemaInfo = this.schemas.get(schemaId);
    if (!schemaInfo) {
      throw new Error(`Schema not found: ${schemaId}`);
    }
    return schemaInfo.schema;
  }
}
```

## ç›‘æ§å’ŒæŒ‡æ ‡

### ç³»ç»ŸæŒ‡æ ‡
```typescript
interface MessageQueueMetrics {
  // KafkaæŒ‡æ ‡
  kafkaTopics: {
    totalTopics: number;
    totalPartitions: number;
    totalMessages: number;
    messagesPerSecond: number;
    bytesPerSecond: number;
  };
  
  // RedisæŒ‡æ ‡
  redisStreams: {
    totalStreams: number;
    totalConsumerGroups: number;
    totalMessages: number;
    pendingMessages: number;
  };
  
  // æ¶ˆè´¹è€…æŒ‡æ ‡
  consumers: {
    activeConsumers: number;
    totalLag: number;
    processingRate: number;
    errorRate: number;
  };
  
  // æ€§èƒ½æŒ‡æ ‡
  performance: {
    averageLatency: number;
    p95Latency: number;
    p99Latency: number;
    throughput: number;
  };
}

@Injectable()
export class MetricsCollector {
  private prometheusRegistry = new Registry();
  
  constructor() {
    this.setupMetrics();
  }
  
  private setupMetrics(): void {
    // æ¶ˆæ¯è®¡æ•°å™¨
    this.messageCounter = new Counter({
      name: 'mq_messages_total',
      help: 'Total number of messages',
      labelNames: ['topic', 'type', 'status', 'tenant_id'],
      registers: [this.prometheusRegistry]
    });
    
    // æ¶ˆæ¯å»¶è¿Ÿç›´æ–¹å›¾
    this.latencyHistogram = new Histogram({
      name: 'mq_message_latency_seconds',
      help: 'Message processing latency',
      labelNames: ['topic', 'handler_type'],
      buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10],
      registers: [this.prometheusRegistry]
    });
    
    // é˜Ÿåˆ—å¤§å°gauge
    this.queueSizeGauge = new Gauge({
      name: 'mq_queue_size',
      help: 'Current queue size',
      labelNames: ['queue_name', 'type'],
      registers: [this.prometheusRegistry]
    });
  }
  
  recordMessage(
    topic: string,
    type: string,
    status: 'published' | 'consumed' | 'failed',
    tenantId: string
  ): void {
    this.messageCounter.inc({
      topic,
      type,
      status,
      tenant_id: tenantId
    });
  }
  
  recordLatency(
    topic: string,
    handlerType: string,
    latencySeconds: number
  ): void {
    this.latencyHistogram.observe(
      { topic, handler_type: handlerType },
      latencySeconds
    );
  }
}
```

### å¥åº·æ£€æŸ¥
```typescript
@Injectable()
export class HealthCheckService {
  async checkKafkaHealth(): Promise<HealthStatus> {
    try {
      const admin = this.kafka.admin();
      await admin.connect();
      
      const metadata = await admin.fetchTopicMetadata();
      await admin.disconnect();
      
      return {
        status: 'healthy',
        details: {
          brokers: metadata.brokers.length,
          topics: metadata.topics.length
        }
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }
  
  async checkRedisHealth(): Promise<HealthStatus> {
    try {
      const pong = await this.redis.ping();
      const info = await this.redis.info('memory');
      
      return {
        status: pong === 'PONG' ? 'healthy' : 'unhealthy',
        details: {
          ping: pong,
          memory: this.parseRedisInfo(info)
        }
      };
    } catch (error) {
      return {
        status: 'unhealthy',
        error: error.message
      };
    }
  }
}
```

## é”™è¯¯å¤„ç†å’Œé‡è¯•

### æ­»ä¿¡é˜Ÿåˆ—
```typescript
@Injectable()
export class DeadLetterQueueService {
  async sendToDeadLetter(
    originalMessage: Message,
    error: Error,
    config: ConsumerConfig
  ): Promise<void> {
    const deadLetterMessage = {
      originalTopic: originalMessage.topic,
      originalPartition: originalMessage.partition,
      originalOffset: originalMessage.offset,
      originalTimestamp: originalMessage.timestamp,
      failureReason: error.message,
      failureStack: error.stack,
      retryCount: originalMessage.retryCount || 0,
      payload: originalMessage.payload,
      headers: originalMessage.headers,
      failedAt: new Date().toISOString()
    };
    
    const deadLetterTopic = config.deadLetterQueue || `${originalMessage.topic}.deadletter`;
    
    await this.publishMessage(deadLetterTopic, deadLetterMessage);
    
    // è®°å½•æ­»ä¿¡æ¶ˆæ¯
    await this.recordDeadLetterMessage(deadLetterMessage);
  }
  
  async reprocessDeadLetterMessages(
    deadLetterTopic: string,
    maxMessages: number = 100
  ): Promise<void> {
    const messages = await this.consumeMessages(deadLetterTopic, maxMessages);
    
    for (const message of messages) {
      try {
        const originalTopic = message.payload.originalTopic;
        const originalPayload = message.payload.payload;
        
        // é‡æ–°å‘å¸ƒåˆ°åŸå§‹ä¸»é¢˜
        await this.publishMessage(originalTopic, originalPayload);
        
        // ç¡®è®¤æ­»ä¿¡æ¶ˆæ¯å¤„ç†å®Œæˆ
        await this.acknowledgeMessage(message);
        
      } catch (error) {
        console.error('Failed to reprocess dead letter message:', error);
      }
    }
  }
}
```

## APIè®¾è®¡è§„èŒƒ

### æ¶ˆæ¯å‘å¸ƒæ¥å£
```typescript
interface PublishMessageRequest {
  topic: string;
  messages: Array<{
    key?: string;
    payload: any;
    headers?: Record<string, string>;
    partition?: number;
    timestamp?: number;
  }>;
  options?: {
    compression?: 'none' | 'gzip' | 'snappy' | 'lz4';
    acks?: 'all' | 0 | 1;
    timeout?: number;
  };
}

interface PublishMessageResponse {
  messageIds: string[];
  partitions: Array<{
    partition: number;
    offset: number;
  }>;
  timestamp: number;
}
```

### é”™è¯¯å“åº”æ ¼å¼
```typescript
enum MessageQueueErrorCode {
  TOPIC_NOT_FOUND = 'TOPIC_NOT_FOUND',
  SUBSCRIPTION_NOT_FOUND = 'SUBSCRIPTION_NOT_FOUND',
  SERIALIZATION_ERROR = 'SERIALIZATION_ERROR',
  SCHEMA_VALIDATION_ERROR = 'SCHEMA_VALIDATION_ERROR',
  QUOTA_EXCEEDED = 'QUOTA_EXCEEDED',
  CONNECTION_ERROR = 'CONNECTION_ERROR',
  BROKER_NOT_AVAILABLE = 'BROKER_NOT_AVAILABLE'
}
```

## éƒ¨ç½²é…ç½®

### ç¯å¢ƒå˜é‡
```env
# Kafkaé…ç½®
KAFKA_BROKERS=localhost:9092,localhost:9093,localhost:9094
KAFKA_CLIENT_ID=message-queue-service
KAFKA_COMPRESSION_TYPE=gzip
KAFKA_BATCH_SIZE=16384
KAFKA_LINGER_MS=5

# Redisé…ç½®
REDIS_CLUSTER_NODES=localhost:7001,localhost:7002,localhost:7003
REDIS_PASSWORD=your_redis_password
REDIS_MAX_RETRIES=3

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://user:password@localhost:5432/messagequeue

# ç›‘æ§é…ç½®
METRICS_ENABLED=true
PROMETHEUS_PORT=9465
HEALTH_CHECK_PORT=8081

# æ€§èƒ½è°ƒä¼˜
MAX_CONCURRENT_CONSUMERS=10
MESSAGE_BATCH_SIZE=100
PROCESSING_TIMEOUT_MS=30000
```

### Docker Composeé…ç½®
```yaml
version: '3.8'
services:
  message-queue-service:
    build: .
    ports:
      - "3010:3010"
      - "9465:9465"
    environment:
      - KAFKA_BROKERS=kafka1:9092,kafka2:9092,kafka3:9092
      - REDIS_CLUSTER_NODES=redis1:6379,redis2:6379,redis3:6379
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - redis1
      - redis2
      - redis3
      
  kafka1:
    image: confluentinc/cp-kafka:latest
    environment:
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka1:9092
      KAFKA_BROKER_ID: 1
      
  redis1:
    image: redis:7-alpine
    command: redis-server --cluster-enabled yes --cluster-config-file nodes.conf --cluster-node-timeout 5000 --appendonly yes
```

## æ€§èƒ½ä¼˜åŒ–

### æ‰¹é‡å¤„ç†ä¼˜åŒ–
```typescript
@Injectable()
export class BatchProcessor {
  private batches: Map<string, MessageBatch> = new Map();
  
  async addToBatch(topic: string, message: Message): Promise<void> {
    let batch = this.batches.get(topic);
    
    if (!batch) {
      batch = new MessageBatch(topic, this.config.batchSize);
      this.batches.set(topic, batch);
    }
    
    batch.addMessage(message);
    
    if (batch.isFull() || batch.isExpired()) {
      await this.processBatch(batch);
      this.batches.delete(topic);
    }
  }
  
  private async processBatch(batch: MessageBatch): Promise<void> {
    try {
      await this.publishBatch(batch.topic, batch.messages);
      batch.markAsProcessed();
    } catch (error) {
      batch.markAsFailed(error);
      await this.handleBatchError(batch, error);
    }
  }
}
```

### è¿æ¥æ± ç®¡ç†
```typescript
@Injectable()
export class ConnectionPoolManager {
  private kafkaProducers: Pool<Producer>;
  private redisConnections: Pool<Redis>;
  
  constructor() {
    this.kafkaProducers = new Pool({
      create: () => this.createKafkaProducer(),
      destroy: (producer) => producer.disconnect(),
      min: 2,
      max: 10,
      acquireTimeoutMillis: 30000,
      idleTimeoutMillis: 300000
    });
    
    this.redisConnections = new Pool({
      create: () => this.createRedisConnection(),
      destroy: (redis) => redis.disconnect(),
      min: 5,
      max: 20,
      acquireTimeoutMillis: 10000,
      idleTimeoutMillis: 60000
    });
  }
  
  async withKafkaProducer<T>(operation: (producer: Producer) => Promise<T>): Promise<T> {
    const producer = await this.kafkaProducers.acquire();
    try {
      return await operation(producer);
    } finally {
      this.kafkaProducers.release(producer);
    }
  }
}
```

## å®‰å…¨è€ƒè™‘

### æ¶ˆæ¯åŠ å¯†
```typescript
@Injectable()
export class MessageEncryption {
  async encryptMessage(payload: any, tenantKey: string): Promise<string> {
    const cipher = crypto.createCipher('aes-256-gcm', tenantKey);
    const encrypted = cipher.update(JSON.stringify(payload), 'utf8', 'hex') + 
                     cipher.final('hex');
    const authTag = cipher.getAuthTag();
    
    return Buffer.from(JSON.stringify({
      data: encrypted,
      authTag: authTag.toString('hex')
    })).toString('base64');
  }
  
  async decryptMessage(encryptedPayload: string, tenantKey: string): Promise<any> {
    const { data, authTag } = JSON.parse(
      Buffer.from(encryptedPayload, 'base64').toString()
    );
    
    const decipher = crypto.createDecipher('aes-256-gcm', tenantKey);
    decipher.setAuthTag(Buffer.from(authTag, 'hex'));
    
    const decrypted = decipher.update(data, 'hex', 'utf8') + decipher.final('utf8');
    return JSON.parse(decrypted);
  }
}
```

### è®¿é—®æ§åˆ¶
```typescript
@Injectable()
export class MessageAccessControl {
  async checkPublishPermission(
    userId: string,
    topic: string,
    tenantId: string
  ): Promise<boolean> {
    const topicConfig = await this.getTopicConfig(topic);
    
    // æ£€æŸ¥ç§Ÿæˆ·æƒé™
    if (topicConfig.tenantId !== tenantId) {
      return false;
    }
    
    // æ£€æŸ¥ç”Ÿäº§è€…æƒé™
    if (topicConfig.allowedProducers.length > 0) {
      return topicConfig.allowedProducers.includes(userId);
    }
    
    return true;
  }
  
  async checkConsumePermission(
    userId: string,
    topic: string,
    tenantId: string
  ): Promise<boolean> {
    const topicConfig = await this.getTopicConfig(topic);
    
    // æ£€æŸ¥ç§Ÿæˆ·æƒé™
    if (topicConfig.tenantId !== tenantId && !topicConfig.isPublic) {
      return false;
    }
    
    // æ£€æŸ¥æ¶ˆè´¹è€…æƒé™
    if (topicConfig.allowedConsumers.length > 0) {
      return topicConfig.allowedConsumers.includes(userId);
    }
    
    return true;
  }
}
```

é€šè¿‡è¿™æ ·çš„è®¾è®¡ï¼Œæ¶ˆæ¯é˜Ÿåˆ—æœåŠ¡èƒ½å¤Ÿæä¾›é«˜æ€§èƒ½ã€é«˜å¯ç”¨ã€å¯æ‰©å±•çš„æ¶ˆæ¯ä¼ é€’èƒ½åŠ›ï¼Œæ»¡è¶³ä¼ä¸šçº§åº”ç”¨çš„å„ç§æ¶ˆæ¯é€šä¿¡éœ€æ±‚ã€‚