# æ–‡ä»¶å­˜å‚¨æœåŠ¡å¼€å‘æ–‡æ¡£ - æ ‡å‡†ç‰ˆæœ¬

## æœåŠ¡æ¦‚è¿°

æ–‡ä»¶å­˜å‚¨æœåŠ¡æ˜¯å¾®æœåŠ¡å¹³å°çš„èµ„æºç®¡ç†æ ¸å¿ƒï¼Œé¢å‘**100ç§Ÿæˆ·+10ä¸‡ç”¨æˆ·**çš„ä¼ä¸šçº§ç”Ÿäº§ç³»ç»Ÿï¼Œè´Ÿè´£æ–‡ä»¶ä¸Šä¼ ã€å­˜å‚¨ã€è®¿é—®æ§åˆ¶ã€CDNåŠ é€Ÿã€åª’ä½“å¤„ç†ç­‰åŠŸèƒ½ï¼Œä¸ºæ•´ä¸ªå¹³å°æä¾›ç»Ÿä¸€çš„æ–‡ä»¶ç®¡ç†èƒ½åŠ›ã€‚

### ğŸ¯ æ ‡å‡†ç‰ˆæœ¬å®šä½
- **å­˜å‚¨è§„æ¨¡**: æ”¯æŒ100TBæ–‡ä»¶å­˜å‚¨ï¼Œ10ä¸‡ç”¨æˆ·æ–‡ä»¶ç®¡ç†
- **ä¸Šä¼ æ€§èƒ½**: æ”¯æŒå¤§æ–‡ä»¶åˆ†ç‰‡ä¸Šä¼ ï¼Œå¹¶å‘ä¸Šä¼ ä¼˜åŒ– 
- **å­˜å‚¨æ–¹æ¡ˆ**: MinIOå¯¹è±¡å­˜å‚¨ + PostgreSQLå…ƒæ•°æ®
- **è®¿é—®æ§åˆ¶**: ç»†ç²’åº¦æƒé™æ§åˆ¶ï¼Œå®‰å…¨è®¿é—®é“¾æ¥
- **éƒ¨ç½²æ–¹å¼**: Docker Compose + MinIOå¯¹è±¡å­˜å‚¨
- **æœåŠ¡ä¼˜å…ˆçº§**: Week 3å¼€å‘ï¼Œå¤æ‚åº¦â­â­â­â­ (10/12)
- **ä¾èµ–æœåŠ¡**: è®¤è¯æœåŠ¡(3001) + æƒé™æœåŠ¡(3002) + ç”¨æˆ·æœåŠ¡(3003)
- **å†…å­˜åˆ†é…**: 512MB (æ ‡å‡†ç‰ˆæœ¬8GBæ€»å†…å­˜ä¸­çš„6.4%)

## æŠ€æœ¯æ ˆ

### åç«¯æŠ€æœ¯
- **æ¡†æ¶**: NestJS 10.x + TypeScript 5.x
- **æ•°æ®åº“**: PostgreSQL 15+ (å…ƒæ•°æ®) + Redis 7+ (ç¼“å­˜)
- **ORM**: Prisma ORM
- **æ–‡ä»¶å­˜å‚¨**: MinIO (S3å…¼å®¹ï¼Œæ ‡å‡†ç‰ˆæœ¬æ¨è) / AWS S3 / æœ¬åœ°å­˜å‚¨
- **å›¾ç‰‡å¤„ç†**: Sharp.js
- **è§†é¢‘å¤„ç†**: FFmpeg
- **å‹ç¼©**: Node.js zlib / 7zip
- **å®‰å…¨æ‰«æ**: ClamAV é˜²ç—…æ¯’

### å­˜å‚¨æŠ€æœ¯ (æ ‡å‡†ç‰ˆæœ¬)
- **å¯¹è±¡å­˜å‚¨**: MinIO (è‡ªéƒ¨ç½²S3å…¼å®¹å­˜å‚¨)
- **CDNåŠ é€Ÿ**: Nginxåå‘ä»£ç† + ç¼“å­˜
- **ç¼“å­˜ç­–ç•¥**: Redis + æµè§ˆå™¨ç¼“å­˜
- **å¤‡ä»½ç­–ç•¥**: æœ¬åœ°å¤‡ä»½ + å®šæœŸå½’æ¡£
- **æ¶ˆæ¯é˜Ÿåˆ—**: Redis Streams (æ›¿ä»£Kafka)
- **éƒ¨ç½²æ–¹æ¡ˆ**: Docker Compose (æ›¿ä»£Kubernetes)

### ç›‘æ§æŠ€æœ¯
- **æ—¥å¿—**: Winston
- **æŒ‡æ ‡**: Prometheus + Grafana
- **å¥åº·æ£€æŸ¥**: NestJS Health Check

## æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

### 1. æ–‡ä»¶å…ƒæ•°æ®ç®¡ç†
```typescript
// æ–‡ä»¶å®ä½“å®šä¹‰
interface FileEntity {
  id: string
  tenantId: string
  userId: string
  filename: string
  originalName: string
  mimeType: string
  size: number
  md5Hash: string
  sha256Hash: string
  storageProvider: 'local' | 's3' | 'oss' | 'cos'
  storagePath: string
  storageKey: string
  bucketName?: string
  isPublic: boolean
  downloadCount: number
  tags: string[]
  metadata: Record<string, any>
  status: 'uploading' | 'processing' | 'ready' | 'failed' | 'deleted'
  expiresAt?: Date
  createdAt: Date
  updatedAt: Date
  deletedAt?: Date
}

// æ–‡ä»¶ç‰ˆæœ¬ç®¡ç†
interface FileVersion {
  id: string
  fileId: string
  version: number
  size: number
  storageKey: string
  metadata: Record<string, any>
  createdAt: Date
}

// æ–‡ä»¶åˆ†äº«é“¾æ¥
interface FileShare {
  id: string
  fileId: string
  shareToken: string
  password?: string
  expiresAt?: Date
  downloadLimit?: number
  downloadCount: number
  allowedIPs?: string[]
  createdBy: string
  createdAt: Date
}
```

### 2. æ–‡ä»¶ä¸Šä¼ å¤„ç†
```typescript
// æ–‡ä»¶ä¸Šä¼ æœåŠ¡
interface FileUploadService {
  uploadSingle(file: Express.Multer.File, options: UploadOptions): Promise<FileEntity>
  uploadMultiple(files: Express.Multer.File[], options: UploadOptions): Promise<FileEntity[]>
  uploadByUrl(url: string, options: UploadOptions): Promise<FileEntity>
  uploadChunked(chunk: ChunkData, options: ChunkUploadOptions): Promise<ChunkUploadResult>
  resumableUpload(resumeToken: string, chunk: ChunkData): Promise<ChunkUploadResult>
  completeChunkedUpload(uploadId: string): Promise<FileEntity>
  cancelUpload(uploadId: string): Promise<void>
}

// åˆ†ç‰‡ä¸Šä¼ å¤„ç†
interface ChunkUploadService {
  initializeUpload(metadata: FileMetadata): Promise<ChunkedUpload>
  uploadChunk(uploadId: string, chunkNumber: number, chunk: Buffer): Promise<ChunkResult>
  completeUpload(uploadId: string): Promise<FileEntity>
  abortUpload(uploadId: string): Promise<void>
  listParts(uploadId: string): Promise<UploadPart[]>
}

// ä¸Šä¼ é…ç½®
interface UploadOptions {
  tenantId: string
  userId: string
  folder?: string
  isPublic?: boolean
  tags?: string[]
  expiresAt?: Date
  allowedMimeTypes?: string[]
  maxFileSize?: number
  autoProcessImages?: boolean
  generateThumbnails?: boolean
  scanForVirus?: boolean
}
```

### 3. å¤šå­˜å‚¨åç«¯æ”¯æŒ
```typescript
// å­˜å‚¨æä¾›å•†æ¥å£
interface StorageProvider {
  upload(key: string, file: Buffer | Stream, options: StorageOptions): Promise<StorageResult>
  download(key: string): Promise<Buffer>
  getDownloadUrl(key: string, expiresIn?: number): Promise<string>
  delete(key: string): Promise<void>
  exists(key: string): Promise<boolean>
  getMetadata(key: string): Promise<StorageMetadata>
  listObjects(prefix: string, options?: ListOptions): Promise<StorageObject[]>
}

// AWS S3 å®ç°
@Injectable()
export class S3StorageProvider implements StorageProvider {
  private s3: AWS.S3

  constructor(private config: S3Config) {
    this.s3 = new AWS.S3({
      accessKeyId: config.accessKeyId,
      secretAccessKey: config.secretAccessKey,
      region: config.region
    })
  }

  async upload(key: string, file: Buffer | Stream, options: StorageOptions): Promise<StorageResult> {
    const uploadParams = {
      Bucket: options.bucket,
      Key: key,
      Body: file,
      ContentType: options.contentType,
      ServerSideEncryption: 'AES256',
      Metadata: options.metadata
    }

    if (options.isPublic) {
      uploadParams['ACL'] = 'public-read'
    }

    const result = await this.s3.upload(uploadParams).promise()
    
    return {
      key: result.Key,
      url: result.Location,
      etag: result.ETag,
      versionId: result.VersionId
    }
  }

  async getDownloadUrl(key: string, expiresIn: number = 3600): Promise<string> {
    return this.s3.getSignedUrl('getObject', {
      Bucket: this.config.bucket,
      Key: key,
      Expires: expiresIn
    })
  }
}

// é˜¿é‡Œäº‘OSSå®ç°
@Injectable()
export class OSSStorageProvider implements StorageProvider {
  private client: OSS

  constructor(private config: OSSConfig) {
    this.client = new OSS({
      region: config.region,
      accessKeyId: config.accessKeyId,
      accessKeySecret: config.accessKeySecret,
      bucket: config.bucket
    })
  }

  // OSS specific implementation
}

// æœ¬åœ°å­˜å‚¨å®ç°
@Injectable()
export class LocalStorageProvider implements StorageProvider {
  constructor(private config: LocalStorageConfig) {}
  
  // Local filesystem implementation
}
```

### 4. åª’ä½“å¤„ç†æœåŠ¡
```typescript
// å›¾ç‰‡å¤„ç†æœåŠ¡
interface ImageProcessingService {
  generateThumbnails(fileId: string, sizes: ImageSize[]): Promise<FileEntity[]>
  resizeImage(fileId: string, width: number, height: number, options?: ResizeOptions): Promise<FileEntity>
  cropImage(fileId: string, x: number, y: number, width: number, height: number): Promise<FileEntity>
  rotateImage(fileId: string, degrees: number): Promise<FileEntity>
  convertFormat(fileId: string, format: 'jpeg' | 'png' | 'webp' | 'avif'): Promise<FileEntity>
  addWatermark(fileId: string, watermarkId: string, options: WatermarkOptions): Promise<FileEntity>
  optimizeImage(fileId: string, quality?: number): Promise<FileEntity>
}

// è§†é¢‘å¤„ç†æœåŠ¡
interface VideoProcessingService {
  generateThumbnail(fileId: string, timestamp: number): Promise<FileEntity>
  extractFrames(fileId: string, intervals: number[]): Promise<FileEntity[]>
  transcodeVideo(fileId: string, format: VideoFormat, quality: VideoQuality): Promise<FileEntity>
  compressVideo(fileId: string, options: CompressionOptions): Promise<FileEntity>
  addSubtitles(fileId: string, subtitleFileId: string): Promise<FileEntity>
  getVideoInfo(fileId: string): Promise<VideoMetadata>
}

// æ–‡æ¡£å¤„ç†æœåŠ¡
interface DocumentProcessingService {
  convertToPdf(fileId: string): Promise<FileEntity>
  generatePreview(fileId: string, pages?: number[]): Promise<FileEntity[]>
  extractText(fileId: string): Promise<string>
  splitDocument(fileId: string, pageRanges: PageRange[]): Promise<FileEntity[]>
  mergeDocuments(fileIds: string[]): Promise<FileEntity>
  addWatermark(fileId: string, watermarkText: string, options: WatermarkOptions): Promise<FileEntity>
}
```

### 5. å®‰å…¨ä¸è®¿é—®æ§åˆ¶
```typescript
// æ–‡ä»¶æƒé™ç®¡ç†
interface FilePermission {
  id: string
  fileId: string
  userId?: string
  roleId?: string
  tenantId: string
  permissions: ('read' | 'write' | 'delete' | 'share')[]
  expiresAt?: Date
  createdAt: Date
}

// è®¿é—®æ§åˆ¶æœåŠ¡
interface AccessControlService {
  checkPermission(userId: string, fileId: string, permission: string): Promise<boolean>
  grantPermission(fileId: string, userId: string, permissions: string[]): Promise<FilePermission>
  revokePermission(fileId: string, userId: string): Promise<void>
  listUserFiles(userId: string, tenantId: string, options?: ListOptions): Promise<FileEntity[]>
  createShareLink(fileId: string, options: ShareOptions): Promise<FileShare>
  validateShareLink(shareToken: string, password?: string): Promise<FileShare>
}

// ç—…æ¯’æ‰«ææœåŠ¡
interface VirusScanService {
  scanFile(fileId: string): Promise<ScanResult>
  quarantineFile(fileId: string, reason: string): Promise<void>
  restoreFile(fileId: string): Promise<void>
  updateVirusDatabase(): Promise<void>
}

// æ‰«æç»“æœ
interface ScanResult {
  fileId: string
  isClean: boolean
  threats: VirusThreat[]
  scanDate: Date
  scanEngine: string
  engineVersion: string
}
```

## æ•°æ®åº“è®¾è®¡

### PostgreSQL è¡¨ç»“æ„
```sql
-- æ–‡ä»¶ä¸»è¡¨
CREATE TABLE files (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id UUID NOT NULL,
  user_id UUID NOT NULL,
  filename VARCHAR(255) NOT NULL,
  original_name VARCHAR(255) NOT NULL,
  mime_type VARCHAR(100) NOT NULL,
  size BIGINT NOT NULL,
  md5_hash CHAR(32) NOT NULL,
  sha256_hash CHAR(64) NOT NULL,
  storage_provider VARCHAR(50) NOT NULL,
  storage_path TEXT NOT NULL,
  storage_key TEXT NOT NULL,
  bucket_name VARCHAR(100),
  is_public BOOLEAN DEFAULT false,
  download_count INTEGER DEFAULT 0,
  tags TEXT[] DEFAULT '{}',
  metadata JSONB DEFAULT '{}',
  status VARCHAR(50) DEFAULT 'ready',
  expires_at TIMESTAMP,
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW(),
  deleted_at TIMESTAMP,
  
  INDEX idx_files_tenant_user (tenant_id, user_id),
  INDEX idx_files_hash (md5_hash, sha256_hash),
  INDEX idx_files_status (status, created_at),
  INDEX idx_files_expires (expires_at) WHERE expires_at IS NOT NULL,
  INDEX idx_files_tags USING GIN (tags),
  UNIQUE (tenant_id, md5_hash, sha256_hash)
);

-- æ–‡ä»¶ç‰ˆæœ¬è¡¨
CREATE TABLE file_versions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
  version INTEGER NOT NULL,
  size BIGINT NOT NULL,
  storage_key TEXT NOT NULL,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP DEFAULT NOW(),
  
  INDEX idx_versions_file (file_id, version),
  UNIQUE (file_id, version)
);

-- æ–‡ä»¶æƒé™è¡¨
CREATE TABLE file_permissions (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
  user_id UUID,
  role_id UUID,
  tenant_id UUID NOT NULL,
  permissions VARCHAR(50)[] NOT NULL,
  expires_at TIMESTAMP,
  created_at TIMESTAMP DEFAULT NOW(),
  
  INDEX idx_permissions_file (file_id),
  INDEX idx_permissions_user (user_id, tenant_id),
  INDEX idx_permissions_expires (expires_at) WHERE expires_at IS NOT NULL
);

-- æ–‡ä»¶åˆ†äº«è¡¨
CREATE TABLE file_shares (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
  share_token VARCHAR(255) NOT NULL UNIQUE,
  password_hash VARCHAR(255),
  expires_at TIMESTAMP,
  download_limit INTEGER,
  download_count INTEGER DEFAULT 0,
  allowed_ips TEXT[],
  created_by UUID NOT NULL,
  created_at TIMESTAMP DEFAULT NOW(),
  
  INDEX idx_shares_token (share_token),
  INDEX idx_shares_file (file_id),
  INDEX idx_shares_expires (expires_at) WHERE expires_at IS NOT NULL
);

-- åˆ†ç‰‡ä¸Šä¼ è¡¨
CREATE TABLE chunked_uploads (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  upload_id VARCHAR(255) NOT NULL UNIQUE,
  tenant_id UUID NOT NULL,
  user_id UUID NOT NULL,
  filename VARCHAR(255) NOT NULL,
  total_size BIGINT NOT NULL,
  chunk_size INTEGER NOT NULL,
  total_chunks INTEGER NOT NULL,
  uploaded_chunks INTEGER DEFAULT 0,
  storage_provider VARCHAR(50) NOT NULL,
  storage_key TEXT NOT NULL,
  metadata JSONB DEFAULT '{}',
  status VARCHAR(50) DEFAULT 'active',
  expires_at TIMESTAMP NOT NULL,
  created_at TIMESTAMP DEFAULT NOW(),
  
  INDEX idx_uploads_user (user_id, tenant_id),
  INDEX idx_uploads_status (status, expires_at)
);

-- ä¸Šä¼ åˆ†ç‰‡è®°å½•è¡¨
CREATE TABLE upload_chunks (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  upload_id UUID NOT NULL REFERENCES chunked_uploads(id) ON DELETE CASCADE,
  chunk_number INTEGER NOT NULL,
  chunk_size INTEGER NOT NULL,
  etag VARCHAR(255),
  uploaded_at TIMESTAMP DEFAULT NOW(),
  
  INDEX idx_chunks_upload (upload_id, chunk_number),
  UNIQUE (upload_id, chunk_number)
);

-- ç—…æ¯’æ‰«æç»“æœè¡¨
CREATE TABLE virus_scan_results (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
  is_clean BOOLEAN NOT NULL,
  threats JSONB DEFAULT '[]',
  scan_date TIMESTAMP DEFAULT NOW(),
  scan_engine VARCHAR(100) NOT NULL,
  engine_version VARCHAR(50) NOT NULL,
  
  INDEX idx_scan_file (file_id),
  INDEX idx_scan_date (scan_date),
  INDEX idx_scan_threats (is_clean)
);

-- æ–‡ä»¶å¤„ç†ä»»åŠ¡è¡¨
CREATE TABLE file_processing_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  file_id UUID NOT NULL REFERENCES files(id) ON DELETE CASCADE,
  job_type VARCHAR(50) NOT NULL,
  job_data JSONB NOT NULL,
  status VARCHAR(50) DEFAULT 'pending',
  result JSONB,
  error_message TEXT,
  started_at TIMESTAMP,
  completed_at TIMESTAMP,
  created_at TIMESTAMP DEFAULT NOW(),
  
  INDEX idx_jobs_file (file_id),
  INDEX idx_jobs_status (status, created_at),
  INDEX idx_jobs_type (job_type, status)
);
```

### Redis æ•°æ®ç»“æ„
```typescript
// ä¸Šä¼ è¿›åº¦ç¼“å­˜
interface RedisUploadCache {
  'upload:progress:{uploadId}': {
    totalChunks: number
    uploadedChunks: number
    totalSize: number
    uploadedSize: number
    lastActivity: number
  }
  
  // ä¸‹è½½é™åˆ¶
  'download:limit:{userId}:{date}': number  // æ¯æ—¥ä¸‹è½½æ¬¡æ•°
  'download:bandwidth:{userId}': number     // å¸¦å®½ä½¿ç”¨é‡
  
  // æ–‡ä»¶ç¼“å­˜
  'file:metadata:{fileId}': FileEntity      // æ–‡ä»¶å…ƒæ•°æ®ç¼“å­˜
  'file:permissions:{userId}:{fileId}': string[]  // æƒé™ç¼“å­˜
  
  // åˆ†äº«é“¾æ¥ç¼“å­˜
  'share:token:{shareToken}': FileShare     // åˆ†äº«é“¾æ¥ä¿¡æ¯
  
  // å¤„ç†ä»»åŠ¡é˜Ÿåˆ—
  'processing:queue': ProcessingJob[]       // æ–‡ä»¶å¤„ç†ä»»åŠ¡é˜Ÿåˆ—
}
```

## API è®¾è®¡

### RESTful API ç«¯ç‚¹
```typescript
// æ–‡ä»¶ä¸Šä¼  API
@Controller('files')
export class FileController {
  @Post('upload')
  @UseInterceptors(FileInterceptor('file'))
  @ApiConsumes('multipart/form-data')
  async uploadFile(
    @UploadedFile() file: Express.Multer.File,
    @Body() options: UploadOptionsDto,
    @Req() req: AuthenticatedRequest
  ) {
    return this.fileService.uploadSingle(file, {
      ...options,
      tenantId: req.tenantId,
      userId: req.user.id
    })
  }

  @Post('upload/multiple')
  @UseInterceptors(FilesInterceptor('files', 10))
  async uploadMultipleFiles(
    @UploadedFiles() files: Express.Multer.File[],
    @Body() options: UploadOptionsDto,
    @Req() req: AuthenticatedRequest
  ) {
    return this.fileService.uploadMultiple(files, {
      ...options,
      tenantId: req.tenantId,
      userId: req.user.id
    })
  }

  @Post('upload/chunked/init')
  async initChunkedUpload(@Body() metadata: ChunkedUploadInitDto) {
    return this.chunkService.initializeUpload(metadata)
  }

  @Post('upload/chunked/:uploadId/chunk/:chunkNumber')
  @UseInterceptors(FileInterceptor('chunk'))
  async uploadChunk(
    @Param('uploadId') uploadId: string,
    @Param('chunkNumber') chunkNumber: number,
    @UploadedFile() chunk: Express.Multer.File
  ) {
    return this.chunkService.uploadChunk(uploadId, chunkNumber, chunk.buffer)
  }

  @Post('upload/chunked/:uploadId/complete')
  async completeChunkedUpload(@Param('uploadId') uploadId: string) {
    return this.chunkService.completeUpload(uploadId)
  }

  @Delete('upload/chunked/:uploadId')
  async abortChunkedUpload(@Param('uploadId') uploadId: string) {
    return this.chunkService.abortUpload(uploadId)
  }
}

// æ–‡ä»¶ç®¡ç† API
@Controller('files')
export class FileManagementController {
  @Get()
  async listFiles(@Query() query: ListFilesDto, @Req() req: AuthenticatedRequest) {
    return this.fileService.listUserFiles(req.user.id, req.tenantId, query)
  }

  @Get(':id')
  async getFile(@Param('id') id: string) {
    return this.fileService.getFileById(id)
  }

  @Get(':id/download')
  async downloadFile(@Param('id') id: string, @Res() res: Response) {
    const downloadUrl = await this.fileService.getDownloadUrl(id)
    res.redirect(downloadUrl)
  }

  @Get(':id/stream')
  async streamFile(@Param('id') id: string, @Res() res: Response) {
    const stream = await this.fileService.getFileStream(id)
    stream.pipe(res)
  }

  @Put(':id')
  async updateFile(@Param('id') id: string, @Body() updateData: UpdateFileDto) {
    return this.fileService.updateFile(id, updateData)
  }

  @Delete(':id')
  async deleteFile(@Param('id') id: string) {
    return this.fileService.deleteFile(id)
  }

  @Post(':id/copy')
  async copyFile(@Param('id') id: string, @Body() options: CopyFileDto) {
    return this.fileService.copyFile(id, options)
  }

  @Post(':id/move')
  async moveFile(@Param('id') id: string, @Body() options: MoveFileDto) {
    return this.fileService.moveFile(id, options)
  }
}

// æ–‡ä»¶åˆ†äº« API
@Controller('shares')
export class FileShareController {
  @Post()
  async createShare(@Body() shareData: CreateShareDto, @Req() req: AuthenticatedRequest) {
    return this.shareService.createShareLink(shareData.fileId, {
      ...shareData,
      createdBy: req.user.id
    })
  }

  @Get(':token')
  async getShareInfo(@Param('token') token: string, @Body() password?: string) {
    return this.shareService.validateShareLink(token, password)
  }

  @Get(':token/download')
  async downloadSharedFile(@Param('token') token: string, @Body() password?: string) {
    const share = await this.shareService.validateShareLink(token, password)
    return this.fileService.getDownloadUrl(share.fileId)
  }

  @Delete(':id')
  async deleteShare(@Param('id') id: string) {
    return this.shareService.deleteShare(id)
  }
}

// åª’ä½“å¤„ç† API
@Controller('media')
export class MediaProcessingController {
  @Post('images/:id/thumbnails')
  async generateThumbnails(@Param('id') id: string, @Body() sizes: ImageSize[]) {
    return this.imageService.generateThumbnails(id, sizes)
  }

  @Post('images/:id/resize')
  async resizeImage(@Param('id') id: string, @Body() options: ResizeImageDto) {
    return this.imageService.resizeImage(id, options.width, options.height, options)
  }

  @Post('images/:id/convert')
  async convertImage(@Param('id') id: string, @Body() options: ConvertImageDto) {
    return this.imageService.convertFormat(id, options.format)
  }

  @Post('videos/:id/thumbnail')
  async generateVideoThumbnail(@Param('id') id: string, @Body() options: VideoThumbnailDto) {
    return this.videoService.generateThumbnail(id, options.timestamp)
  }

  @Post('videos/:id/transcode')
  async transcodeVideo(@Param('id') id: string, @Body() options: TranscodeVideoDto) {
    return this.videoService.transcodeVideo(id, options.format, options.quality)
  }

  @Post('documents/:id/convert')
  async convertDocument(@Param('id') id: string, @Body() options: ConvertDocumentDto) {
    return this.documentService.convertToPdf(id)
  }
}
```

## å­˜å‚¨ç­–ç•¥é…ç½®

### å¤šå­˜å‚¨åç«¯é…ç½®
```typescript
// å­˜å‚¨é…ç½®æ¥å£
interface StorageConfiguration {
  default: string  // é»˜è®¤å­˜å‚¨åç«¯
  backends: {
    local?: LocalStorageConfig
    s3?: S3StorageConfig
    oss?: OSSStorageConfig
    cos?: COSStorageConfig
  }
  rules: StorageRule[]  // å­˜å‚¨è§„åˆ™
}

// å­˜å‚¨è§„åˆ™
interface StorageRule {
  name: string
  conditions: {
    mimeType?: string[]
    size?: { min?: number, max?: number }
    tenantId?: string[]
    userRole?: string[]
  }
  backend: string
  options?: {
    encryption?: boolean
    compression?: boolean
    replication?: boolean
    ttl?: number
  }
}

// ç¤ºä¾‹é…ç½®
const storageConfig: StorageConfiguration = {
  default: 's3',
  backends: {
    local: {
      path: '/var/lib/file-storage',
      maxSize: '10GB'
    },
    s3: {
      accessKeyId: process.env.AWS_ACCESS_KEY_ID,
      secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
      region: 'us-east-1',
      bucket: 'my-app-files'
    },
    oss: {
      accessKeyId: process.env.ALIBABA_ACCESS_KEY_ID,
      accessKeySecret: process.env.ALIBABA_ACCESS_KEY_SECRET,
      region: 'oss-cn-hangzhou',
      bucket: 'my-app-files'
    }
  },
  rules: [
    {
      name: 'large-videos',
      conditions: {
        mimeType: ['video/*'],
        size: { min: 100 * 1024 * 1024 } // > 100MB
      },
      backend: 'oss',
      options: {
        compression: true,
        replication: true
      }
    },
    {
      name: 'temporary-files',
      conditions: {
        size: { max: 10 * 1024 * 1024 } // < 10MB
      },
      backend: 'local',
      options: {
        ttl: 24 * 60 * 60 // 24å°æ—¶è¿‡æœŸ
      }
    }
  ]
}
```

## ğŸ“‹ é¡¹ç›®è§„åˆ’ (æ ‡å‡†ç‰ˆæœ¬)

### å¼€å‘é‡Œç¨‹ç¢‘

**Week 3 å¼€å‘è®¡åˆ’** (æ–‡ä»¶å­˜å‚¨æœåŠ¡ä¼˜å…ˆçº§: 10/12)

#### é‡Œç¨‹ç¢‘ 1: åŸºç¡€æ¶æ„ (Week 3.1-3.2)
- âœ… PostgreSQLè¡¨ç»“æ„è®¾è®¡
- âœ… MinIOå¯¹è±¡å­˜å‚¨é›†æˆ
- âœ… åŸºç¡€CRUD APIå¼€å‘
- âœ… æ–‡ä»¶ä¸Šä¼ ä¸‹è½½åŠŸèƒ½
- ç›®æ ‡ï¼šåŸºç¡€æ–‡ä»¶ç®¡ç†åŠŸèƒ½å¯ç”¨

#### é‡Œç¨‹ç¢‘ 2: é«˜çº§åŠŸèƒ½ (Week 3.3-3.4)
- ğŸ”„ åˆ†ç‰‡ä¸Šä¼ æ”¯æŒ
- ğŸ”„ æƒé™æ§åˆ¶é›†æˆ
- ğŸ”„ åª’ä½“å¤„ç†åŠŸèƒ½
- ğŸ”„ å®‰å…¨æ‰«æé›†æˆ
- ç›®æ ‡ï¼šä¼ä¸šçº§åŠŸèƒ½å®Œå¤‡

#### é‡Œç¨‹ç¢‘ 3: é›†æˆæµ‹è¯• (Week 3.5-3.6)
- ğŸ”„ ä¸è®¤è¯æœåŠ¡é›†æˆ
- ğŸ”„ ä¸æƒé™æœåŠ¡é›†æˆ
- ğŸ”„ æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
- ğŸ”„ ç›‘æ§å‘Šè­¦é…ç½®
- ç›®æ ‡ï¼šç”Ÿäº§å°±ç»ª

#### é‡Œç¨‹ç¢‘ 4: éƒ¨ç½²ä¸Šçº¿ (Week 3.7)
- ğŸ”„ Docker Composeé…ç½®
- ğŸ”„ ç¯å¢ƒå˜é‡é…ç½®
- ğŸ”„ å¤‡ä»½ç­–ç•¥å®æ–½
- ğŸ”„ æ–‡æ¡£å®Œå–„
- ç›®æ ‡ï¼šç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

### å†…å­˜åˆ†é…ç­–ç•¥

**æ ‡å‡†ç‰ˆæœ¬æ€»å†…å­˜é¢„ç®—: 8GB**

| ç»„ä»¶ | å†…å­˜åˆ†é… | å æ¯” | è¯´æ˜ |
|------|---------|------|------|
| æ–‡ä»¶å­˜å‚¨æœåŠ¡ | 512MB | 6.4% | åŸºç¡€è¿è¡Œå†…å­˜ |
| MinIOå¯¹è±¡å­˜å‚¨ | 512MB | 6.4% | å¯¹è±¡å­˜å‚¨å¼•æ“ |
| PostgreSQLå…±äº« | 2GB | 25% | æ‰€æœ‰æœåŠ¡å…±äº«æ•°æ®åº“ |
| Rediså…±äº« | 512MB | 6.4% | ç¼“å­˜å’Œä¼šè¯å­˜å‚¨ |
| Nginxä»£ç† | 128MB | 1.6% | é™æ€æ–‡ä»¶æœåŠ¡ |
| å…¶ä»–æœåŠ¡ | 4.3GB | 54.2% | å‰©ä½™11ä¸ªå¾®æœåŠ¡ |

### é£é™©è¯„ä¼°

#### æŠ€æœ¯é£é™© ğŸ”´
- **å¤§æ–‡ä»¶ä¸Šä¼ å¤„ç†**: åˆ†ç‰‡ä¸Šä¼ å¤æ‚æ€§ï¼Œå†…å­˜å ç”¨æ§åˆ¶
- **å­˜å‚¨æ€§èƒ½**: MinIOåœ¨å•æœºç¯å¢ƒä¸‹çš„æ€§èƒ½è¡¨ç°
- **ç¼“è§£æªæ–½**: ä½¿ç”¨æµå¼å¤„ç†ï¼Œé™åˆ¶å•æ–‡ä»¶å¤§å°ï¼Œå®æ–½æ–‡ä»¶å‹ç¼©

#### ä¾èµ–é£é™© ğŸŸ¡
- **è®¤è¯æœåŠ¡ä¾èµ–**: æ–‡ä»¶æƒé™æ§åˆ¶ä¾èµ–è®¤è¯æœåŠ¡
- **æƒé™æœåŠ¡ä¾èµ–**: ç»†ç²’åº¦è®¿é—®æ§åˆ¶ä¾èµ–RBACæœåŠ¡
- **ç¼“è§£æªæ–½**: å®ç°é™çº§æ¨¡å¼ï¼Œæœ¬åœ°æƒé™ç¼“å­˜

#### é›†æˆé£é™© ğŸŸ¡
- **MinIOé›†æˆå¤æ‚æ€§**: S3å…¼å®¹APIçš„é…ç½®å’Œè°ƒè¯•
- **åª’ä½“å¤„ç†æ€§èƒ½**: Sharp/FFmpegåœ¨å®¹å™¨ç¯å¢ƒçš„èµ„æºæ¶ˆè€—
- **ç¼“è§£æªæ–½**: è¯¦ç»†çš„é›†æˆæµ‹è¯•ï¼Œæ€§èƒ½åŸºå‡†æµ‹è¯•

#### è¿ç»´é£é™© ğŸŸ¢
- **å­˜å‚¨ç©ºé—´ç®¡ç†**: 100TBå­˜å‚¨ç©ºé—´çš„ç›‘æ§å’Œæ¸…ç†
- **å¤‡ä»½æ¢å¤**: å¤§æ–‡ä»¶å¤‡ä»½ç­–ç•¥çš„å¯é æ€§
- **ç¼“è§£æªæ–½**: è‡ªåŠ¨åŒ–æ¸…ç†ä»»åŠ¡ï¼Œå¢é‡å¤‡ä»½ç­–ç•¥

### æœåŠ¡é—´äº¤äº’è®¾è®¡

#### ä¾èµ–å…³ç³»å›¾
```
æ–‡ä»¶å­˜å‚¨æœåŠ¡ (3006)
    â†“ ç”¨æˆ·èº«ä»½éªŒè¯
è®¤è¯æˆæƒæœåŠ¡ (3001)
    â†“ æƒé™æ£€æŸ¥
æƒé™ç®¡ç†æœåŠ¡ (3002)
    â†“ ç”¨æˆ·ä¿¡æ¯
ç”¨æˆ·ç®¡ç†æœåŠ¡ (3003)
    â†“ å®¡è®¡æ—¥å¿—
æ—¥å¿—å®¡è®¡æœåŠ¡ (3008)
    â†“ ä»»åŠ¡è°ƒåº¦
ä»»åŠ¡è°ƒåº¦æœåŠ¡ (3009)
```

#### å†…éƒ¨APIæ¥å£è®¾è®¡

**æœåŠ¡é—´è®¤è¯**: ä½¿ç”¨X-Service-Tokenå¤´éƒ¨
```typescript
// å†…éƒ¨æœåŠ¡è°ƒç”¨ç¤ºä¾‹
headers: {
  'X-Service-Token': process.env.INTERNAL_SERVICE_TOKEN,
  'X-Tenant-ID': tenantId,
  'X-User-ID': userId
}
```

**æ ¸å¿ƒå†…éƒ¨ç«¯ç‚¹**:
- `POST /internal/files/validate` - éªŒè¯æ–‡ä»¶æƒé™
- `GET /internal/files/{id}/metadata` - è·å–æ–‡ä»¶å…ƒæ•°æ®
- `POST /internal/files/{id}/audit` - è®°å½•è®¿é—®æ—¥å¿—
- `GET /internal/files/usage/{tenantId}` - è·å–ç§Ÿæˆ·å­˜å‚¨ä½¿ç”¨é‡

## éƒ¨ç½²æ–¹æ¡ˆ

### Docker Compose é…ç½® (æ ‡å‡†ç‰ˆæœ¬)

```yaml
# docker-compose.yml
version: '3.8'

services:
  file-storage-service:
    build:
      context: ./file-storage-service
      dockerfile: Dockerfile
    container_name: file-storage-service
    ports:
      - "3006:3006"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://platform_user:platform_pass@postgres:5432/platform_db
      - REDIS_URL=redis://redis:6379
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_ENDPOINT=minio:9000
      - MAX_FILE_SIZE=100MB
      - ALLOWED_MIME_TYPES=image/*,video/*,application/pdf,text/*
    volumes:
      - file_storage_data:/app/storage
      - ./logs:/app/logs
    depends_on:
      - postgres
      - redis
      - minio
    networks:
      - platform-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3006/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # MinIOå¯¹è±¡å­˜å‚¨ (æ ‡å‡†ç‰ˆæœ¬)
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-minioadmin}
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    networks:
      - platform-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'

  # Nginxæ–‡ä»¶æœåŠ¡ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: nginx-file-proxy
    ports:
      - "8080:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - file_storage_data:/var/www/files:ro
    depends_on:
      - file-storage-service
    networks:
      - platform-network

volumes:
  file_storage_data:
    driver: local
  minio_data:
    driver: local

networks:
  platform-network:
    external: true
```

### Nginx é…ç½®
```nginx
# nginx/nginx.conf
events {
    worker_connections 1024;
}

http {
    upstream file_storage {
        server file-storage-service:3006;
    }
    
    # æ–‡ä»¶ç›´æ¥è®¿é—®
    server {
        listen 80;
        
        # é™æ€æ–‡ä»¶æœåŠ¡
        location /files/ {
            alias /var/www/files/;
            expires 1d;
            add_header Cache-Control "public, immutable";
        }
        
        # APIä»£ç†
        location /api/ {
            proxy_pass http://file_storage;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # å¤§æ–‡ä»¶ä¸Šä¼ é…ç½®
            client_max_body_size 100M;
            proxy_read_timeout 300s;
            proxy_send_timeout 300s;
        }
    }
}
```

## ç›‘æ§å‘Šè­¦

### å…³é”®æŒ‡æ ‡
```typescript
// Prometheus æŒ‡æ ‡å®šä¹‰
const metrics = {
  // æ–‡ä»¶ä¸Šä¼ æŒ‡æ ‡
  filesUploaded: new promClient.Counter({
    name: 'files_uploaded_total',
    help: 'Total number of files uploaded',
    labelNames: ['tenant_id', 'storage_backend', 'mime_type']
  }),
  
  // æ–‡ä»¶å¤§å°æŒ‡æ ‡
  uploadSize: new promClient.Histogram({
    name: 'file_upload_size_bytes',
    help: 'File upload size distribution',
    labelNames: ['storage_backend'],
    buckets: [1024, 10240, 102400, 1048576, 10485760, 104857600] // 1KB to 100MB
  }),
  
  // å¤„ç†æ—¶é—´æŒ‡æ ‡
  processingDuration: new promClient.Histogram({
    name: 'file_processing_duration_seconds',
    help: 'File processing duration',
    labelNames: ['operation', 'mime_type'],
    buckets: [0.1, 0.5, 1, 5, 10, 30, 60]
  }),
  
  // å­˜å‚¨ä½¿ç”¨é‡
  storageUsage: new promClient.Gauge({
    name: 'storage_usage_bytes',
    help: 'Current storage usage',
    labelNames: ['tenant_id', 'storage_backend']
  }),
  
  // ä¸‹è½½ç»Ÿè®¡
  downloads: new promClient.Counter({
    name: 'file_downloads_total',
    help: 'Total number of file downloads',
    labelNames: ['tenant_id', 'access_type'] // direct, shared
  })
}
```

### å‘Šè­¦è§„åˆ™
```yaml
# prometheus-alerts.yml
groups:
- name: file-storage-service
  rules:
  - alert: HighUploadFailureRate
    expr: rate(files_uploaded_total{status="failed"}[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
    annotations:
      summary: "High file upload failure rate"
      description: "Upload failure rate is {{ $value }} for {{ $labels.storage_backend }}"

  - alert: StorageQuotaExceeded
    expr: storage_usage_bytes / storage_quota_bytes > 0.9
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Storage quota nearly exceeded"
      description: "Tenant {{ $labels.tenant_id }} is using {{ $value | humanizePercentage }} of storage quota"

  - alert: SlowFileProcessing
    expr: histogram_quantile(0.95, rate(file_processing_duration_seconds_bucket[5m])) > 60
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "File processing too slow"
      description: "95th percentile processing time is {{ $value }}s for {{ $labels.operation }}"

  - alert: VirusScanFailure
    expr: increase(virus_scan_failures_total[1h]) > 10
    for: 0m
    labels:
      severity: critical
    annotations:
      summary: "Multiple virus scan failures"
      description: "{{ $value }} virus scan failures in the last hour"
```

## å¼€å‘æŒ‡å—

### æœ¬åœ°å¼€å‘ç¯å¢ƒ
```bash
# 1. å…‹éš†é¡¹ç›®
git clone <repository-url>
cd file-storage-service

# 2. å®‰è£…ä¾èµ–
npm install

# 3. å¯åŠ¨ä¾èµ–æœåŠ¡
docker-compose up -d postgres redis minio

# 4. æ•°æ®åº“è¿ç§»
npx prisma migrate dev

# 5. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
npm run start:dev

# 6. è¿è¡Œæµ‹è¯•
npm run test
npm run test:e2e
```

### ç¯å¢ƒå˜é‡é…ç½®
```bash
# .env.development
DATABASE_URL="postgresql://user:pass@localhost:5432/file_storage_db"
REDIS_URL="redis://localhost:6379"

# å­˜å‚¨é…ç½®
STORAGE_DEFAULT_BACKEND="local"
STORAGE_LOCAL_PATH="/tmp/file-storage"

# AWS S3 é…ç½®
AWS_ACCESS_KEY_ID="your-access-key"
AWS_SECRET_ACCESS_KEY="your-secret-key"
AWS_REGION="us-east-1"
AWS_S3_BUCKET="your-bucket"

# æ–‡ä»¶é™åˆ¶
MAX_FILE_SIZE="100MB"
ALLOWED_MIME_TYPES="image/*,video/*,application/pdf"

# å®‰å…¨é…ç½®
ENABLE_VIRUS_SCAN="true"
ENABLE_FILE_ENCRYPTION="true"

# CDNé…ç½®
CDN_DOMAIN="https://cdn.yourdomain.com"
CDN_ENABLED="true"
```

### Docker Composeé›†æˆé…ç½®

```yaml
# æ ‡å‡†ç‰ˆæœ¬æ ¸å¿ƒé…ç½®
services:
  file-storage-service:
    # æœåŠ¡å‘ç°: ä½¿ç”¨æœåŠ¡å file-storage-service:3006
    networks:
      - platform-network
    depends_on:
      - postgres      # å…±äº«PostgreSQLå®ä¾‹
      - redis         # å…±äº«Rediså®ä¾‹
      - minio         # ä¸“ç”¨MinIOå®ä¾‹
    # å¥åº·æ£€æŸ¥é›†æˆ
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3006/health"]
```

### ç›‘æ§é›†æˆ (Prometheus + Grafana)

```typescript
// å…³é”®ä¸šåŠ¡æŒ‡æ ‡
const fileStorageMetrics = {
  // å­˜å‚¨ä½¿ç”¨é‡æŒ‡æ ‡
  storageUsed: new Gauge({
    name: 'file_storage_used_bytes',
    help: 'Total storage used by tenant',
    labelNames: ['tenant_id']
  }),
  
  // ä¸Šä¼ æ€§èƒ½æŒ‡æ ‡
  uploadLatency: new Histogram({
    name: 'file_upload_duration_seconds',
    help: 'File upload duration',
    labelNames: ['file_size_range', 'storage_backend']
  }),
  
  // æœåŠ¡å¯ç”¨æ€§æŒ‡æ ‡
  serviceHealth: new Gauge({
    name: 'file_storage_service_up',
    help: 'File storage service availability'
  })
}
```

è¿™ä¸ªæ–‡ä»¶å­˜å‚¨æœåŠ¡å°†ä¸ºæ•´ä¸ªå¾®æœåŠ¡å¹³å°æä¾›å¼ºå¤§çš„æ–‡ä»¶ç®¡ç†èƒ½åŠ›ï¼Œæ”¯æŒå¤šç§å­˜å‚¨åç«¯ã€åª’ä½“å¤„ç†ã€å®‰å…¨æ‰«æç­‰ä¼ä¸šçº§åŠŸèƒ½ï¼Œå®Œå…¨ç¬¦åˆæ ‡å‡†ç‰ˆæœ¬çš„æŠ€æœ¯é€‰å‹å’Œæ€§èƒ½è¦æ±‚ã€‚